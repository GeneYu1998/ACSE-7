{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.optional {\n",
       "    display: block;\n",
       "    background-color: #d7e2ff;\n",
       "    border-color: #d7e2ff;\n",
       "    border-left: 5px solid #d7e2ff;\n",
       "    padding: 0.5em;\n",
       "}\n",
       "div.advanced {\n",
       "    display: block;\n",
       "    background-color: #fff4d7;\n",
       "    border-color: #fff4d7;\n",
       "    border-left: 5px solid #fff4d7;\n",
       "    padding: 0.5em;\n",
       "}\n",
       "div.text_cell_render{\n",
       "    font-size:14pt;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.optional {\n",
    "    display: block;\n",
    "    background-color: #d7e2ff;\n",
    "    border-color: #d7e2ff;\n",
    "    border-left: 5px solid #d7e2ff;\n",
    "    padding: 0.5em;\n",
    "}\n",
    "div.advanced {\n",
    "    display: block;\n",
    "    background-color: #fff4d7;\n",
    "    border-color: #fff4d7;\n",
    "    border-left: 5px solid #fff4d7;\n",
    "    padding: 0.5em;\n",
    "}\n",
    "div.text_cell_render{\n",
    "    font-size:14pt;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACSE-7 (Optimisation and Inversion) <a class=\"tocSkip\">    \n",
    "\n",
    "## Lecture 1: Introduction and methods for small linear problems  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives  <a class=\"tocSkip\">\n",
    "   \n",
    "    \n",
    "    \n",
    "1. To review some key linear algebra results (a review of some of ACSE-2)\n",
    "\n",
    "    \n",
    "2. To extend our linear algebra knowledge to non-square systems\n",
    "\n",
    "    \n",
    "3. To introduce concepts underlying the computational tasks of optimisation and inversion\n",
    "\n",
    "    \n",
    "4. To appreciate equi, under, over and mixed determined systems, and some of the techniques that can be used to deal with them\n",
    "    \n",
    "\n",
    "5. To introduce the Singular Value Decomposition (SVD) of a matrix \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "[Note that some of the starred optional sections below are reviews of material we've covered previously, so we will skim over them quickly, they're included here for completeness.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introductory-comments---Inversion-&amp;-Optimisation\" data-toc-modified-id=\"Introductory-comments---Inversion-&amp;-Optimisation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introductory comments - Inversion &amp; Optimisation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inversion\" data-toc-modified-id=\"Inversion-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Inversion</a></span><ul class=\"toc-item\"><li><span><a href=\"#A-simple-scalar-linear-example\" data-toc-modified-id=\"A-simple-scalar-linear-example-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>A simple scalar linear example</a></span></li><li><span><a href=\"#A-simple-scalar-nonlinear-example\" data-toc-modified-id=\"A-simple-scalar-nonlinear-example-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>A simple scalar nonlinear example</a></span></li><li><span><a href=\"#A-reminder-on-Newton's-method-[*]\" data-toc-modified-id=\"A-reminder-on-Newton's-method-[*]-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>A reminder on Newton's method [*]</a></span></li><li><span><a href=\"#Multi-dimensional-and-nonlinear-problems\" data-toc-modified-id=\"Multi-dimensional-and-nonlinear-problems-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Multi-dimensional and nonlinear problems</a></span></li><li><span><a href=\"#Terminology\" data-toc-modified-id=\"Terminology-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>Terminology</a></span></li><li><span><a href=\"#A-simple-inversion/parameter-estimation-example:-polynomial-interpolation/least-squares-curve-fitting\" data-toc-modified-id=\"A-simple-inversion/parameter-estimation-example:-polynomial-interpolation/least-squares-curve-fitting-1.1.6\"><span class=\"toc-item-num\">1.1.6&nbsp;&nbsp;</span>A simple inversion/parameter estimation example: polynomial interpolation/least squares curve-fitting</a></span></li><li><span><a href=\"#The-least-squares-solution\" data-toc-modified-id=\"The-least-squares-solution-1.1.7\"><span class=\"toc-item-num\">1.1.7&nbsp;&nbsp;</span>The least squares solution</a></span></li><li><span><a href=\"#A-more-complex-Inversion-example\" data-toc-modified-id=\"A-more-complex-Inversion-example-1.1.8\"><span class=\"toc-item-num\">1.1.8&nbsp;&nbsp;</span>A more complex Inversion example</a></span></li></ul></li><li><span><a href=\"#Optimisation\" data-toc-modified-id=\"Optimisation-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Optimisation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimisation-as-an-inversion-problem\" data-toc-modified-id=\"Optimisation-as-an-inversion-problem-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Optimisation as an inversion problem</a></span></li><li><span><a href=\"#Inversion-as-an-optimisation-problem\" data-toc-modified-id=\"Inversion-as-an-optimisation-problem-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Inversion as an optimisation problem</a></span></li><li><span><a href=\"#Optimisation---simple-example\" data-toc-modified-id=\"Optimisation---simple-example-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Optimisation - simple example</a></span></li><li><span><a href=\"#Optimisation-(more-complex)\" data-toc-modified-id=\"Optimisation-(more-complex)-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>Optimisation (more complex)</a></span></li></ul></li></ul></li><li><span><a href=\"#More-on-forward-and-inverse-problems--[*]\" data-toc-modified-id=\"More-on-forward-and-inverse-problems--[*]-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>More on forward and inverse problems  [*]</a></span><ul class=\"toc-item\"><li><span><a href=\"#Abstract-problem-setting\" data-toc-modified-id=\"Abstract-problem-setting-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Abstract problem setting</a></span></li><li><span><a href=\"#Real-world-problems\" data-toc-modified-id=\"Real-world-problems-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Real world problems</a></span></li><li><span><a href=\"#Examples\" data-toc-modified-id=\"Examples-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Examples</a></span></li></ul></li><li><span><a href=\"#Linear-algebra-review-[*]\" data-toc-modified-id=\"Linear-algebra-review-[*]-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Linear algebra review [*]</a></span><ul class=\"toc-item\"><li><span><a href=\"#Quick-reminder-on-matrix-notation-&amp;-terminology\" data-toc-modified-id=\"Quick-reminder-on-matrix-notation-&amp;-terminology-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Quick reminder on matrix notation &amp; terminology</a></span></li><li><span><a href=\"#Linear-systems\" data-toc-modified-id=\"Linear-systems-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Linear systems</a></span></li><li><span><a href=\"#Expanding-the-geometrical-thinking\" data-toc-modified-id=\"Expanding-the-geometrical-thinking-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Expanding the geometrical thinking</a></span></li><li><span><a href=\"#Solvability-of-linear-systems\" data-toc-modified-id=\"Solvability-of-linear-systems-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Solvability of linear systems</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-(in)dependence\" data-toc-modified-id=\"Linear-(in)dependence-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Linear (in)dependence</a></span></li><li><span><a href=\"#A-basis-for-a-vector-space\" data-toc-modified-id=\"A-basis-for-a-vector-space-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>A basis for a vector space</a></span></li><li><span><a href=\"#An-example-|-the-geometrical-interpretation-of-matrix-vector-multiplication\" data-toc-modified-id=\"An-example-|-the-geometrical-interpretation-of-matrix-vector-multiplication-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>An example | the geometrical interpretation of matrix vector multiplication</a></span></li></ul></li><li><span><a href=\"#Solvability-theory---some-equivalent-properties\" data-toc-modified-id=\"Solvability-theory---some-equivalent-properties-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Solvability theory - some equivalent properties</a></span></li></ul></li><li><span><a href=\"#Rank,-range-and-null-space\" data-toc-modified-id=\"Rank,-range-and-null-space-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Rank, range and null-space</a></span><ul class=\"toc-item\"><li><span><a href=\"#Rank-and-range-for-square-systems\" data-toc-modified-id=\"Rank-and-range-for-square-systems-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Rank and range for square systems</a></span></li><li><span><a href=\"#Rank-and-range-in-the-non-square-case\" data-toc-modified-id=\"Rank-and-range-in-the-non-square-case-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Rank and range in the non-square case</a></span></li><li><span><a href=\"#The-null-space\" data-toc-modified-id=\"The-null-space-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>The null-space</a></span></li><li><span><a href=\"#The-rank-nullity-theorem\" data-toc-modified-id=\"The-rank-nullity-theorem-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>The rank-nullity theorem</a></span></li><li><span><a href=\"#(Reduced)-row-echelon-form-of-a-matrix-and-links-to-the-matrix-rank-and-null-space\" data-toc-modified-id=\"(Reduced)-row-echelon-form-of-a-matrix-and-links-to-the-matrix-rank-and-null-space-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>(Reduced) row echelon form of a matrix and links to the matrix rank and null space</a></span><ul class=\"toc-item\"><li><span><a href=\"#Calculating-the-rank-and-the-null-space\" data-toc-modified-id=\"Calculating-the-rank-and-the-null-space-4.5.1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>Calculating the rank and the null space</a></span></li></ul></li></ul></li><li><span><a href=\"#An-overview-of-over-,-under-,-equi--and-mixed-determined-problems\" data-toc-modified-id=\"An-overview-of-over-,-under-,-equi--and-mixed-determined-problems-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>An overview of over-, under-, equi- and mixed-determined problems</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-equi-determined-case\" data-toc-modified-id=\"The-equi-determined-case-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>The equi-determined case</a></span><ul class=\"toc-item\"><li><span><a href=\"#Aside---the-Rouché–Capelli-theorem\" data-toc-modified-id=\"Aside---the-Rouché–Capelli-theorem-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Aside - the Rouché–Capelli theorem</a></span></li></ul></li><li><span><a href=\"#The-over-determined-case\" data-toc-modified-id=\"The-over-determined-case-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>The over-determined case</a></span></li><li><span><a href=\"#The-under-determined-case\" data-toc-modified-id=\"The-under-determined-case-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>The under-determined case</a></span></li><li><span><a href=\"#The-mixed-determined-case\" data-toc-modified-id=\"The-mixed-determined-case-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>The mixed-determined case</a></span></li><li><span><a href=\"#Some-examples\" data-toc-modified-id=\"Some-examples-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Some examples</a></span></li></ul></li><li><span><a href=\"#Equi-determined-problems---square-linear-systems\" data-toc-modified-id=\"Equi-determined-problems---square-linear-systems-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Equi-determined problems - square linear systems</a></span></li><li><span><a href=\"#Over-determined-problems\" data-toc-modified-id=\"Over-determined-problems-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Over-determined problems</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-least-squares-solution---derivation-[*]\" data-toc-modified-id=\"The-least-squares-solution---derivation-[*]-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>The least squares solution - derivation [*]</a></span></li></ul></li><li><span><a href=\"#Under-determined-problems\" data-toc-modified-id=\"Under-determined-problems-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Under-determined problems</a></span><ul class=\"toc-item\"><li><span><a href=\"#Derivation-of-the-minimum-norm-solution-[*]\" data-toc-modified-id=\"Derivation-of-the-minimum-norm-solution-[*]-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Derivation of the minimum norm solution [*]</a></span></li></ul></li><li><span><a href=\"#Mixed-determined-problems\" data-toc-modified-id=\"Mixed-determined-problems-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Mixed-determined problems</a></span><ul class=\"toc-item\"><li><span><a href=\"#Possible-solution-methods\" data-toc-modified-id=\"Possible-solution-methods-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Possible solution methods</a></span></li></ul></li><li><span><a href=\"#Matrix-diagonalisation---eigenvalue-decomposition-reminder-[*]\" data-toc-modified-id=\"Matrix-diagonalisation---eigenvalue-decomposition-reminder-[*]-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Matrix diagonalisation - eigenvalue decomposition reminder [*]</a></span><ul class=\"toc-item\"><li><span><a href=\"#Some-useful-theoretical-results-about-eigenvalues/vectors\" data-toc-modified-id=\"Some-useful-theoretical-results-about-eigenvalues/vectors-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Some useful theoretical results about eigenvalues/vectors</a></span></li><li><span><a href=\"#Matrix-diagonalisation\" data-toc-modified-id=\"Matrix-diagonalisation-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Matrix diagonalisation</a></span></li><li><span><a href=\"#A-geometrical-interpretation\" data-toc-modified-id=\"A-geometrical-interpretation-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>A geometrical interpretation</a></span></li></ul></li><li><span><a href=\"#Singular-Value-Decomposition-(SVD)\" data-toc-modified-id=\"Singular-Value-Decomposition-(SVD)-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Singular Value Decomposition (SVD)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Some-SVD-properties-[*]\" data-toc-modified-id=\"Some-SVD-properties-[*]-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Some SVD properties [*]</a></span></li><li><span><a href=\"#Geometric-interpretation\" data-toc-modified-id=\"Geometric-interpretation-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>Geometric interpretation</a></span></li><li><span><a href=\"#The-compact-form-of-SVD\" data-toc-modified-id=\"The-compact-form-of-SVD-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;</span>The compact form of SVD</a></span></li><li><span><a href=\"#Aside:-some-of-the-various-uses-of-the-SVD\" data-toc-modified-id=\"Aside:-some-of-the-various-uses-of-the-SVD-11.4\"><span class=\"toc-item-num\">11.4&nbsp;&nbsp;</span>Aside: some of the various uses of the SVD</a></span></li></ul></li><li><span><a href=\"#The-generalised/pseudo/Moore-Penrose-inverse\" data-toc-modified-id=\"The-generalised/pseudo/Moore-Penrose-inverse-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>The generalised/pseudo/Moore-Penrose inverse</a></span><ul class=\"toc-item\"><li><span><a href=\"#Definition\" data-toc-modified-id=\"Definition-12.1\"><span class=\"toc-item-num\">12.1&nbsp;&nbsp;</span>Definition</a></span></li><li><span><a href=\"#Properties\" data-toc-modified-id=\"Properties-12.2\"><span class=\"toc-item-num\">12.2&nbsp;&nbsp;</span>Properties</a></span></li><li><span><a href=\"#Problems-[*]\" data-toc-modified-id=\"Problems-[*]-12.3\"><span class=\"toc-item-num\">12.3&nbsp;&nbsp;</span>Problems [*]</a></span></li></ul></li><li><span><a href=\"#Regularisation-[*]\" data-toc-modified-id=\"Regularisation-[*]-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Regularisation [*]</a></span><ul class=\"toc-item\"><li><span><a href=\"#Damped-least-squares-[*]\" data-toc-modified-id=\"Damped-least-squares-[*]-13.1\"><span class=\"toc-item-num\">13.1&nbsp;&nbsp;</span>Damped least squares [*]</a></span></li><li><span><a href=\"#More-general-regularisation-[*]\" data-toc-modified-id=\"More-general-regularisation-[*]-13.2\"><span class=\"toc-item-num\">13.2&nbsp;&nbsp;</span>More general regularisation [*]</a></span></li></ul></li><li><span><a href=\"#Nonlinear-problems-[*]\" data-toc-modified-id=\"Nonlinear-problems-[*]-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Nonlinear problems [*]</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%precision 6\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as sl\n",
    "import scipy.optimize as sop\n",
    "from pprint import pprint\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory comments - Inversion & Optimisation\n",
    "\n",
    "Many numerical problems in science, engineering, economics and other quantitative subjects can be posed as ***inversion*** or ***optimisation*** problems.    \n",
    "\n",
    "We will start our discussion by illustrating these problems in a single real variable where the concepts will already be somewhat familiar, before moving on in this lecture to methods that take an analytical linear algebra solution approach before you cover numerical based solution approaches in later lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion\n",
    "\n",
    "Inversion can be thought of as seeking to use a given relationship such as the following \n",
    "\n",
    "$$y = f(x)$$\n",
    "\n",
    "to find $x$ given (1) a value for $y$ and (2) the form of the relationship embodied here in the function $f$ or a means to evaluate $f$.  \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- The **forward problem** can be thought of as: given $x$ what is $y$?\n",
    "\n",
    "\n",
    "- The **inverse problem** is the opposite: given $y$ what is $x$?\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "[When posing an inverse problem we will often swap the sides of the equation, i.e. $f(x)=y$, so that *known* information is on the right and all *unknowns* are on the left - cf. the linear case we are already familiar with: $A\\boldsymbol{x}=\\boldsymbol{b}$].\n",
    "\n",
    "<br>\n",
    "\n",
    "The first is in principle an easy task to achieve as we have access to $f$ (although $f$ may be very expensive and time consuming to evaluate - e.g. a complex numerical model), the second is hard as in general we do not have ready access to $f$'s inverse.\n",
    "\n",
    "This inversion problem has no general solution, and its solution may be trivial, easy, difficult or impossible depending upon the form of $f$ and the value of $y$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple scalar linear example\n",
    "\n",
    "For example, the solution to a linear version of this problem, \n",
    "\n",
    "[i.e. for the given forward model\n",
    "\n",
    "$$y = ax$$\n",
    "\n",
    "given an $x$ what is $y$?] \n",
    "\n",
    "is \n",
    "\n",
    "$$ x = \\frac{1}{a}y$$\n",
    "\n",
    "where the operation of multiplying by $1/a$ is clearly the inverse of multiplying by $a$.\n",
    "\n",
    "We call *multiplying by a* our **forward model** and *multiplying by* $1/a$ is then the corresponding **inverse model**.  \n",
    "\n",
    "This solution is easy, but note that it is not completely trivial since the solution will not work, or is not defined, if $a = 0$; in that case, there is no solution, unless $y=0$ in which case there are infinitely many solutions.\n",
    "\n",
    "We could chose to write the inverse equation as\n",
    "\n",
    "$$ x = (a)^{-1} y$$\n",
    "\n",
    "which makes the inverse relationship explicit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple scalar nonlinear example\n",
    "\n",
    "A nonlinear example of this problem is given by the model\n",
    "\n",
    "$$ y = \\cos(x) $$\n",
    "\n",
    "This of course has the solution \n",
    "\n",
    "$$x = \\cos^{-1}(y) $$\n",
    "\n",
    "But suppose we didn't know or have a way to compute $\\cos^{-1}\\equiv \\arccos$.\n",
    "\n",
    "<br>\n",
    "\n",
    "For this case, assuming that $x$ and $y$ are real numbers, notice that there is no solution if $|y|>1$, and there are infinitely many solutions otherwise.  \n",
    "\n",
    "Despite the lack of a unique solution, the solutions that we obtain are still meaningful and are likely to be useful in practical problems.  \n",
    "\n",
    "In many real problems, we are likely to have additional *a priori* information that will allow us to select one or more solutions from the infinite collection.  \n",
    "\n",
    "For example, we may know that possible solutions are constrained such that $0\\leq x\\leq \\pi$ , which together with our forward model ($y=\\cos(x)$), is sufficient to provide a unique solution (provided $|y|\\le 1$).    \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "In general, the solution to our original equation ($y = f(x)$) can be written as\n",
    "\n",
    "$$ x = f^{-1}(y)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$f\\left[f^{-1}(y)\\right] = y$$\n",
    "\n",
    "for all values of $y$ for which the inverse is defined.\n",
    "\n",
    "There is no general solution to this problem.  It is easy to solve (in the scalar case) if $f$ is a linear function, and a much wider range of problems will have solutions we can find robustly (but maybe not cheaply) as long as we are able to begin from a reasonably good guess.  \n",
    "\n",
    "<br>\n",
    "\n",
    "Newton's method we saw in the Numerical Methods module iteratively solves the problem $\\hat{f}(x)=0$ (I'm introducing a hat here to emphasise that this is a different function to the $f$ above, unless $y=0$ of course) from a starting guess, and can be used for multi-dimensional (i.e. vector valued) problems as well as scalar problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "### A reminder on Newton's method [*]\n",
    "\n",
    "#### For finding roots of equations  <a class=\"tocSkip\">\n",
    "\n",
    "Recall that given a problem written in the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x)=0\n",
    "\\end{equation}\n",
    "\n",
    "Newton's method finds a solution $x$ (termed a root as the RHS of this equation is zero) via the iteration\n",
    "\n",
    "\\begin{equation}\n",
    "x_{i+1} = x_i - \\frac{\\hat{f}(x_i)}{\\hat{f}'(x_i)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We wrote our own code to solve this in the Numerical Methods module, but let's remind ourselves how to use the in built SciPy function:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example f hat function we want to find a/the roots of\n",
    "def fh(x):\n",
    "    return x - np.exp(-x)\n",
    "\n",
    "# its derivative\n",
    "def dfhdx(x):\n",
    "    return 1 + np.exp(-x)\n",
    "\n",
    "x0 = -1. # initial guess\n",
    "# print out what scipy's newton function returns\n",
    "print(sop.newton(fh, x0, dfhdx))\n",
    "\n",
    "\n",
    "# and if you don't have the derivative function it will use a quasi-Newton approach:\n",
    "print(sop.newton(fh, x0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "#### For inversion  <a class=\"tocSkip\">\n",
    "\n",
    "How do we turn our inversion problem into something Newton's method is appropriate to solve?\n",
    "\n",
    "Suppose we want to solve the inversion problem from above where our forward model is described by the equation\n",
    "\n",
    "$$ y = \\cos(x) $$\n",
    "\n",
    "and we have a given output of the model $y=y_d$ ($d$ for given data).\n",
    "\n",
    "Given these two pieces of information, what is $x$?\n",
    "\n",
    "Let's first plot our situation for a particular value of $y_d$.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax1 = plt.subplot(111)\n",
    "\n",
    "x = np.linspace(-10,10,1000)\n",
    "y = np.cos(x)\n",
    "# plot the cos(x) part\n",
    "ax1.plot(x, y, 'b', label=r'$y=\\cos(x)$')\n",
    "# plot the y value we have (our data)\n",
    "yd = 0.5\n",
    "xlim = ax1.get_xlim()\n",
    "ax1.plot([xlim[0], xlim[1]], [yd, yd], 'k--', label='$y=y_d$')\n",
    "ax1.set_ylim([-1.1,1.1])\n",
    "ax1.set_xlabel('$x$', fontsize=16)\n",
    "ax1.set_ylabel('$y$', fontsize=16)\n",
    "ax1.set_title('Nonlinear inversion - scalar example', fontsize=16)\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "If you think about this plot you should be able to see that we can specify inversion problems very easily that have no solutions (non-existence), or infinitely many (non-uniqueness) solutions - these are very possible scenarios to encounter, not just a mathematical peculiarity.\n",
    "\n",
    "To solve this problem using Newton's method we just need to formulate it in a way that this method works with, i.e. as a root-finding problem:\n",
    "\n",
    "let's define the function we want to find the root of as the one whose solution is also a solution to our inversion problem, e.g.\n",
    "\n",
    "$$\\hat{f}:=y_d - \\cos(x)$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yd = 0.5\n",
    "    \n",
    "def fh(x):\n",
    "    return yd - np.cos(x)\n",
    "\n",
    "def dfhdx(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "# initial guess\n",
    "x0 = 1.0\n",
    "print(sop.newton(fh, x0, dfhdx))\n",
    "\n",
    "\n",
    "# and if you don't have the derivative function it will use a quasi-Newton approach:\n",
    "print(sop.newton(fh, x0))\n",
    "\n",
    "# check our solution, i.e. check that cos of this value equal yd = 0.5\n",
    "print(np.allclose( yd,  np.cos(sop.newton(fh, x0))  ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "Can you remember what Newton's iterative updates do?\n",
    "\n",
    "Why would an initial guess of 0 be bad, what about a very small value for the initial guess?\n",
    "\n",
    "Test in the code above.\n",
    "\n",
    "Can you come up with some data (i.e. a $y_d$ value) such that this problem has no solution, what does the Newton function do in this case?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "#### In multi-dimensions  <a class=\"tocSkip\">\n",
    "\n",
    "Note that we will be interested in multi-dimensional problems in this module. Newton's method does generalise to multi-dimensional problems - the division by the derivative at each iteration is replaced by the need to solve a linear system comprising the Jacobian matrix - but this is expensive and so a number of more efficient nonlinear optimisation algorithms have been developed (cf. later for the equivalence between inversion and optimisation problems).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-dimensional and nonlinear problems\n",
    "\n",
    "In this module, we will look at methods that can be used to solve the multi-dimensional analogue of our original problem, that is find a solution for the vector $\\boldsymbol{x}$ given the equation  \n",
    "\n",
    "$$\\boldsymbol{y} = F(\\boldsymbol{x})$$\n",
    "\n",
    "and in this module it will be vital that we consider scenarios where  $\\boldsymbol{x}$ and $\\boldsymbol{y}$ may be of different lengths, i.e. live in spaces with different numbers of dimensions (note this was **not** the cases we considered in ACSE2/3), either of which may also be very large, and where $F$ is an operator that maps a  vector with the dimensions of $\\boldsymbol{x}$ into a vector with the dimensions of $\\boldsymbol{y}$.  \n",
    "\n",
    "In practice, we will also seldom have an analytical form for $F$.\n",
    "Also in general $F$ will not be a matrix; rather it is a (typically non-linear) function that turns one vector into another. You can think of it also as a large complicated numerical model/computer code that calculates $\\boldsymbol{y}$ for us given a particular value for $\\boldsymbol{x}$.\n",
    "\n",
    "\n",
    "However, also note that numerical approaches to solve completely general problems such as this often approximate it as a series of linear/matrix problems (i.e. solve a nonlinear problem by considering a series of linear problems - note this is also what Newton's method does), and that is why we focus in this lecture on situations where our inversion problem can be written as a linear system.   \n",
    "\n",
    "Also, key concepts on inversion can be more easily understood in the linear/matrix system, but these concepts are still highly applicable to/representative of the nonlinear case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "#### \"Parameter estimation\" vs \"inversion\"  <a class=\"tocSkip\">\n",
    "\n",
    "Note that in the above abstract discussion, if we consider $\\boldsymbol{y}$ to be a vector of observations (data), and $\\boldsymbol{x}$ to be a vector of *model parameters* that control a model ($F$), then the problem: given $\\boldsymbol{y}$ find $\\boldsymbol{x}$ is often called a *parameter estimation problem*.\n",
    "\n",
    "Note that the $\\boldsymbol{x}$ we want to invert for, or estimate, doesn't have to include ALL parameters that govern the model, i.e. all inputs to the model. \n",
    "\n",
    "In this case you may see notation something like\n",
    "\n",
    "$$F(\\boldsymbol{x}_1;\\boldsymbol{x}_2) = \\boldsymbol{y}$$\n",
    "\n",
    "where the semi-colon is used to help differentiate different types of variables/parameters that are input into the model, e.g. those that we wish to invert for and those that will remain fixed during the inversion - we'll see some concrete examples below.\n",
    "\n",
    "Sometimes when we have a relatively small number of \"parameters\" to invert for people use the term \"parameter estimation\", but when there are a large number, e.g. representing an entire unknown field, people tend to use the language \"inverse problem\", but it's really the same thing.\n",
    "\n",
    "\n",
    "    \n",
    "<br>\n",
    "    \n",
    "    \n",
    "#### \"Model\" vs \"parameters\"  <a class=\"tocSkip\">\n",
    "\n",
    "Note that different communities use the term *model* in different contexts.\n",
    "\n",
    "Mathematicians/computational scientists would probably refer to $F$ (or $\\boldsymbol{y} = F(\\boldsymbol{x})$) as the \"model\", e.g. the underlying equations (or their discretisation in a computer code), and $\\boldsymbol{x}$ as \"parameters\".\n",
    "\n",
    "Others would refer to $F$ as the operator (and think of it as a given, and in a sense not a \"model\"; they may also refer to $F(\\boldsymbol{x})=\\boldsymbol{y}$ as the \"mathematical model\"), and would reserve the word \"model\" for the parameters $\\boldsymbol{x}$ being inverted for. \n",
    "\n",
    "In seismic inversion problems for example this is because it represents a \"model\" for the real geological make-up of the subsurface.  \n",
    "\n",
    "In fluid dynamics we would probably call this a \"parametrisation\" of some unknown, uncertain or unresolved physical process - again \"parameter\"!\n",
    "\n",
    "In what follows I will generally try to stick to the language that $\\boldsymbol{y}=F(\\boldsymbol{x})$ is the model (e.g. a wave equation or the Navier-Stokes equations and specifically their implementation in code), with $ \\boldsymbol{y}$ the data (or variables that can be post-processed into a form that can be compared to observational data) and *some of* the $\\boldsymbol{x}$ the parameters. \n",
    "\n",
    "<br>\n",
    "\n",
    "However, moving forward I will use common notation for the linear(ised) case we will mostly consider:\n",
    "\n",
    "$$A\\boldsymbol{x}=\\boldsymbol{b}$$\n",
    "\n",
    "where $A$ is a matrix representing the linear *model*, $\\boldsymbol{b}$ is *data* and $\\boldsymbol{x}$ are *parameters* we are inverting for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple inversion/parameter estimation example: polynomial interpolation/least squares curve-fitting\n",
    "\n",
    "A standard parameter estimation problem is fitting (in some sense) a function, defined by a vector of parameters, to a given dataset. We saw this example in *ACSE-2* and *ACSE-3*.\n",
    "\n",
    "\n",
    "Suppose that our \"model\" is given by the polynomial (of degree $N$) function\n",
    "\n",
    "$$f(x; \\boldsymbol{a} ) = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \\ldots a_N x^N$$\n",
    "\n",
    "The vector of $N+1$ parameters $\\boldsymbol{a}$ fully describes our model, and in this case our model is linear in these parameters (and hence the process of finding the parameters is sometimes termed [linear regression](https://en.wikipedia.org/wiki/Linear_regression)). \n",
    "\n",
    "Given some data, we will see that we end up with a ***linear*** problem to solve, even though our model is nonlinear in the parameter(s) $x$ we do **not** invert for. \n",
    "\n",
    "You can think of $x$ as locations we evaluate our model at in order to compare against or match to data, while $\\boldsymbol{a}$ are unknown input parameters to our model that describe in some sense the governing \"physics\".\n",
    "\n",
    "\n",
    "For the inversion problem we are assuming we know the $x$'s (e.g. maybe these are the locations our data is collected at) and the $y$'s (the observations of the output of our problem/model at these locations), and we want to find $\\boldsymbol{a}$ (i.e. the parameters that govern our model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems leading to a square linear/matrix system - fitting a quadratic to three data points  <a class=\"tocSkip\">\n",
    "\n",
    "In *ACSE-3* we considered cases where we  are given $N+1$ data points $(x_i, y_i)$ (with distinct $x_i$'s) we can solve it for $\\boldsymbol{a}$ and thus fix/fully define our \"model\". We called this ***polynomial interpolation***.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "For example, consider a case where we have three data points $\\{(x_0,y_0),(x_1,y_1),(x_2,y_2)\\}$. \n",
    "\n",
    "First we note that the quadratic function has three free parameters and can fit this data exactly.\n",
    "\n",
    "Substituting the data into our quadratic model\n",
    "\n",
    "$$ y = f(x; \\boldsymbol{a} ) \\equiv a_0 + a_1\\,x + a_2\\,x^2$$\n",
    "\n",
    "leads to a system of three simultaneous equations for our three unknowns, or a $3\\times 3$ **linear** system to solve:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & x_0 & x_0^2 \\\\\n",
    "1 & x_1 & x_1^2 \\\\\n",
    "1 & x_2 & x_2^2\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a_0\\\\\n",
    "a_1\\\\\n",
    "a_2\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "y_0\\\\\n",
    "y_1\\\\\n",
    "y_2\n",
    "\\end{pmatrix} \\;\\;\\;\\;\\;\\;\\;\\text{or equivalently in matrix notation} \\;\\;\\;\\;\\;\\; V\\boldsymbol{a} =\\boldsymbol{y}.\n",
    "$$\n",
    "\n",
    "If we solve this system by inverting the matrix ($V$) we have our quadratic polynomial coefficients:  $\\boldsymbol{a} = V^{-1}\\boldsymbol{y}$.  Although recall that for large problems we prefer in general to use a linear \"solver\" to obtain the solution rather than explicitly forming the inverse matrix.\n",
    "\n",
    "Let's form and solve the matrix system using appropriate NumPy functions.\n",
    "\n",
    "The matrix $V$ is what's called the Vandermonde matrix and there is a Numpy function for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_raw_data(xi, yi, ax):\n",
    "    \"\"\"plot x vs y on axes ax, \n",
    "    add axes labels and turn on grid\n",
    "    \"\"\"\n",
    "    ax.plot(xi, yi, 'ko', label='raw data')\n",
    "    ax.set_xlabel('$x$', fontsize=16)\n",
    "    ax.set_ylabel('$y$', fontsize=16)\n",
    "    ax.grid(True)\n",
    "\n",
    "# three data point example\n",
    "xi = [0.3, 0.5, 0.8]\n",
    "yi = [0.2, 0.8, 0.6]\n",
    "\n",
    "# use a function to construct the matrix above\n",
    "# note than numpy already has a function to do this\n",
    "V = np.vander(xi, increasing=True)\n",
    "\n",
    "print('V = \\n{}'.format(V))\n",
    "\n",
    "# use a numpy linear algebra solver to solve the system\n",
    "# uses an LU algorithm - see ACSE 3 for details!\n",
    "a = np.linalg.solve(V, yi)\n",
    "\n",
    "# output the coefficients for our quadratic we have computed\n",
    "print('\\n Our coefficients a = \\n{}\\n'.format(a))\n",
    "\n",
    "# show that they are the same as is obtained from \n",
    "# numpy's polyfit function (for a quadratic)\n",
    "# (which of course they should be, given we argued that this polynomial is unique)\n",
    "print('The output from np.polyfit(x, y, 2) = \\n {}'.format(np.polyfit(xi, yi, 2)))\n",
    "\n",
    "# Note that the order is reversed because numpy.poly* assumes decreasing\n",
    "# rather than the increasing powers of x which we have used above\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# plot the raw data\n",
    "plot_raw_data(xi, yi, ax1)\n",
    "\n",
    "# x locations at which to evaluate and plot the quadratic polynomial\n",
    "x = np.linspace(0., 1., 100)\n",
    "\n",
    "# Set up a polynomial from the coefficients using numpy rather than writing out.\n",
    "# Use numpy.flip to reverse the coefficients as poly1d assume decreasing rather than\n",
    "# increasing powers - look at documentation\n",
    "p2 = np.poly1d(np.flip(a, 0))\n",
    "print('\\nWhich agrees with us as long as we reverse the order of our coefficients:')\n",
    "print('np.flip(a, 0) = \\n{}'.format(np.flip(a, 0)))\n",
    "\n",
    "# the p2 here is a function so evaluate it at our x locations\n",
    "y = p2(x)\n",
    "\n",
    "# and plot\n",
    "ax1.plot(x, y, 'b', label='Quadratic')\n",
    "\n",
    "# add a figure title\n",
    "ax1.set_title('Polynomial approx to three data points', fontsize=16)\n",
    "# Add a legend\n",
    "ax1.legend(loc='best', fontsize=14)\n",
    "# set bounds\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation reconciliation  <a class=\"tocSkip\">\n",
    "\n",
    "Note that to reconcile this with the notation we introduced earlier ...\n",
    "\n",
    "Our scalar problem \n",
    "\n",
    "$$ y = f(x; \\boldsymbol{a} ) \\equiv a_0 + a_1\\,x + a_2\\,x^2$$\n",
    "\n",
    "and data points $\\{(x_0,y_0),(x_1,y_1),(x_2,y_2)\\}$ have been combined into the system\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\n",
    "y_0\\\\\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\n",
    "\\end{pmatrix} &= \n",
    "\\begin{pmatrix}\n",
    "f(x_0; \\boldsymbol{a} )\\\\\n",
    "f(x_1; \\boldsymbol{a} )\\\\\n",
    "f(x_2; \\boldsymbol{a} )\\\\\n",
    "\\vdots\n",
    "\\end{pmatrix}\\\\\n",
    "\\iff \n",
    "\\boldsymbol{y} &= \\boldsymbol{f}(\\boldsymbol{x}; \\boldsymbol{a} ) \\\\\n",
    "&= V\\boldsymbol{a}\n",
    "\\end{align}\n",
    "\n",
    "which is of course equivalent to the notation\n",
    "\n",
    "$$ A \\boldsymbol{x} = \\boldsymbol{b} $$\n",
    "\n",
    "where the data is $\\boldsymbol{b}=\\boldsymbol{y}$, the model parameters are $\\boldsymbol{x}=\\boldsymbol{a}$, and where the governing \"model\" is linear and given by the Vandermonde matrix ($A=V$). \n",
    "\n",
    "So even though our original model $y = f(x; \\boldsymbol{a} )$ wasn't in this form, by considering all of the data together we do end up needing to solve a linear (in $\\boldsymbol{a}$) problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem leading to a non-square linear/matrix system - an example over-determined problem  <a class=\"tocSkip\">\n",
    "\n",
    "In this module it is very important that we understand and are able to deal with situations that result in non-square problems.\n",
    "\n",
    "As an example, let's suppose we have a case where our model is again quadratic, i.e. it has three model parameters:\n",
    "\n",
    "$$ y = f(x; \\boldsymbol{a} ) \\equiv a_0 + a_1\\,x + a_2\\,x^2,$$\n",
    "\n",
    "but let's further suppose we have been given *six* distinct pieces of data, i.e. we now have a mismatch - more data than unknowns.\n",
    "\n",
    "We can write our problem mathematically as\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & x_0 & x_0^2 \\\\\n",
    "1 & x_1 & x_1^2 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_5 & x_5^2\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a_0\\\\\n",
    "a_1\\\\\n",
    "a_2\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "y_0\\\\\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "y_3\\\\\n",
    "y_4\\\\\n",
    "y_5\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "or equivalently $V\\boldsymbol{a} =\\boldsymbol{y}$ where now we have a ***non-square** version of the Vandermonde matrix (read the `numpy.vander` docs).\n",
    "\n",
    "In both the initial problem description, as well as this linear/matrix system, what does it mean to be a solution to this problem?\n",
    "\n",
    "Consider the simpler case of a *linear* (in $x$) model asked to pass through *three* data points. If the three points happened to lie along a line then we can solve the problem, i.e. we can invert for the model parameters. But if not there is no solution (at least no *exact* solution) that fits all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The least squares solution\n",
    "\n",
    "In the language of ACSE-2 we are in the world of curve-fitting (or regression) rather than interpolation - we can't find a model (curve) that fits all the data exactly, but can still obtain a potentially useful model if we appropriately define \"useful\".\n",
    "\n",
    "Previously we used the command `numpy.polyfit` to fit a polynomial to data where the polynomial wasn't of a high enough degree to go through all the data points (or some of the data points weren't at distinct $x$ values - i.e. two $y$ values for the same $x$, think a cloud of points).\n",
    "\n",
    "Instead of requiring the curve to go through all data points exactly (as for the cases above), we used ***least squares*** fitting to construct a function $f(x;\\boldsymbol{a})$, e.g. a polynomial in $x$ of degree $N$, (equivalently finding the parameters $\\boldsymbol{a}$  which minimises the sum of the squares of the differences between $M+1$ data points provided and the polynomial approximation, i.e. it minimises the quantity:\n",
    "\n",
    "$$E := \\sum_{i=0}^{M} (f(x_i;\\boldsymbol{a}) - y_i)^2,$$\n",
    "\n",
    "where $f(x_i;\\boldsymbol{a})$ is the output of our model evaluated at point $x_i$, and the $y_i$ are the corresponding data values.  Note that for the \"cloud of points\" example we will be in the case where $M+1 > N$.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b0/Linear_least_squares_example2.svg\" style=\"width: 300px\">\n",
    "\n",
    "*(Wikipedia: https://en.wikipedia.org/wiki/Linear_least_squares) We're computing the sum of the squares of the distances indicated in green.*\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Note that this $E$ can be defined equivalently as\n",
    "\n",
    "$$E=\\| \\boldsymbol{f}(\\boldsymbol{x}; \\boldsymbol{a} ) - \\boldsymbol{y}\\|_2^2\n",
    "= || V\\boldsymbol{a} - \\boldsymbol{y}||_2^2,$$\n",
    "\n",
    "where $\\| \\cdot \\|_2$ is the 2 norm  or the Euclidean norm.\n",
    "\n",
    "[Why the square of $E$ here? As the 2 norm (Euclidean norm) includes a square root. We know that $E$ can't be negative (why?) and so minimising $E$ is completely equivalent to minimising $E^2$, i.e. we are minimising the 2 norm of the difference between the observed data and the model's prediction of the data.].\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "We will show later that the coefficients ($\\boldsymbol{a}$) of the polynomial that minimises $E$ are given by the solution to the linear system\n",
    "\n",
    "$$V^TV\\boldsymbol{a} = V^T\\boldsymbol{y},$$\n",
    "\n",
    "where $V$ is again the Vandermonde matrix. \n",
    "\n",
    "<br>\n",
    "\n",
    "[As we have seen, $V$ is no longer square in the case where $M+1 > N$. \n",
    "What is the shape of $V$, and indeed all the vectors and matrices appearing in this equation - is it \"dimensionally\" consistent?]\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's check that this is true by forming and solving the matrix system $V^TV\\boldsymbol{a} = V^T\\boldsymbol{y}$ for $\\boldsymbol{a}$ and comparing with the result we get using `numpy.polyfit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Some random data\n",
    "x = np.array([0.5, 2.0, 4.0, 5.0, 7.0, 9.0])\n",
    "y = np.array([0.5, 0.4, 0.3, 0.1, 0.9, 0.8])\n",
    "\n",
    "# Consider a polynomial of degree 3 - so not high enough to go through all the data\n",
    "# (unless we're in the unlikely case where all the data happens to lie on a cubic!)\n",
    "N = 3\n",
    "\n",
    "# Use a numpy function to construct the Vandermonde matrix\n",
    "V = np.vander(x, N+1, increasing=True)\n",
    "\n",
    "# Form the matrix A by transposing V and multiplying by V:\n",
    "A = V.T @ V  # same as A = np.transpose(V) @ V\n",
    "\n",
    "# Use a function from SciPy's linalg sub-package to find the inverse:\n",
    "invA = sl.inv(A)\n",
    "\n",
    "# Form the RHS vector:\n",
    "rhs = V.T @ y\n",
    "\n",
    "# Multipy through by the inverse matrix to find a:\n",
    "a = invA @ rhs\n",
    "print('a = \\n', a)\n",
    "\n",
    "# Compare against the coefficient that numpy's polyfit gives us\n",
    "poly_coeffs = np.polyfit(x, y, N)\n",
    "print('\\npoly_coeffs = \\n', poly_coeffs)\n",
    "# they're the same, we just set up our algorithm to return the coefficient in the \n",
    "# opposite order to polyfit - we need to remember this when we evaluate the polynomial\n",
    "\n",
    "print('\\nOur a vector = output from np.polyfit (as long as we flip the order)? ', \n",
    "      np.allclose(np.flip(a), poly_coeffs))\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.margins(0.1)\n",
    "\n",
    "xx = np.linspace(0.4, 9.1, 100)\n",
    "yy = a[0] + a[1]*xx + a[2] * xx**2 + a[3] * xx**3 \n",
    "\n",
    "ax1.plot(xx, yy, 'b', label='Least squares fit (cubic)')\n",
    "\n",
    "# Overlay raw data\n",
    "ax1.plot(x, y, 'ko', label='Raw data')\n",
    "ax1.set_xlabel('$x$', fontsize=16)\n",
    "ax1.set_ylabel('$y$', fontsize=16)\n",
    "ax1.set_title('Least squares approximation of a cubic to multiple data points', fontsize=16)\n",
    "ax1.grid(True)\n",
    "ax1.legend(loc='best', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key result was that solving the problem\n",
    "\n",
    "$$V^TV\\boldsymbol{a} = V^T\\boldsymbol{y}$$\n",
    "\n",
    "i.e. either using a linear solver to solve this linear system for $\\boldsymbol{a}$, or from computing the inverse and evaluating\n",
    "\n",
    "$$\\boldsymbol{a} = (V^TV)^{-1}V^T\\boldsymbol{y}$$\n",
    "\n",
    "gave us a solution for the parameters which had the property that it fit the data with the minimum least-squares error.\n",
    "\n",
    "It can be shown that if the matrix (here $V$, later $A$) is of full \"column rank\" (the columns are all linearly independent; for square matrices full column rank == full row rank, but note that this is not the case for non-square matrices) then $(V^TV)^{-1}$ exists.\n",
    "\n",
    "We will return to the least squares approximation later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more complex Inversion example\n",
    "\n",
    "In this example from some current research we have a model for tides in the Bristol Channel and Severn Estuary, there are multiple numerical and physical \"parameters\" that go into the model, including incoming tidal boundary conditions, bathymetry etc, some of which are known to varying levels of uncertainty. \n",
    "\n",
    "In this example we consider bed roughness (or bottom friction) as the parameters we wish to invert for, given the data of time series of tidal heights at tide gauges indicated by the red dots in the following image which also shows the discretised domain and the computational mesh\n",
    "\n",
    "<img src=\"./figures/mesh.png\" width=500x>\n",
    "\n",
    "Using *a priori* information on the approximate distribution of sediment grain sizes on the seabed, we partition the domain into three zones: rock, gravel and sand, and assign one parameter value to each. In this case we therefore have more data than parameters.\n",
    "\n",
    "In this case an iterative approach is taken and the following figure shows how the inversion progresses\n",
    "\n",
    "<img src=\"./figures/thetis_inversion_convergence.png\" width=400x>\n",
    "\n",
    "$J$ is a misfit function between modelled tides and observations from real work tide gauges, and we solve the inversion problem by seeking to minimse $J$ - we see it drop as the model fit to data improves with iteration. How the three parameter values vary with iteration is also shown.\n",
    "\n",
    "The quality of the prediction can then be compared at the green squares where data was not used in the inversion. We find that the error in  prediction at these independent locations is indeed reduced within the same numerical model. We have also found that the prediction is improved when these physical values are used in a completely independent tidal model. This latter fact gives us some indication that our inversion is telling us something useful physically and not simply correcting for biases in the first model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "\n",
    "Now let's quickly review what ***optimisation*** is an how it relates to inversion.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Optimisation, in a single real variable, seeks to find a value for $x$ such that a scalar function \n",
    "\n",
    "$$f(x)$$\n",
    "\n",
    "has a minimum value (or equivalently that $-f(x)$ has a maximum value).    \n",
    "\n",
    "For example, $f(x)$   might  be  the  energy  of  a  particle  at  position  $x$. The  value  of  $x$  for  which  this  is  a  minimum will provide a rest position for the particle. \n",
    "\n",
    "Or we could have a problem where we are seeking a design of something that minimises cost or maximises profit.\n",
    "\n",
    "There  are  many  ways  to  solve  such  problems,  and  again  there  is  no  general  solution.  In practical real problems, there may be no solution, a single unique solution, several solutions  or infinitely many solutions.  \n",
    "\n",
    "<br>\n",
    "\n",
    "### Optimisation as an inversion problem\n",
    "\n",
    "One way to address the optimisation problem is to try to solve \n",
    "\n",
    "$$f'(x)=0$$\n",
    "\n",
    "With this approach note that  **we have converted an optimisation problem into an inversion problem** (we are looking for the $x$ values that satisfy this equation - this will give us the [stationary points](https://en.wikipedia.org/wiki/Stationary_point) of the problem).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Inversion as an optimisation problem\n",
    "\n",
    "In practice, it is often the case that in order **to solve an inversion  problem, it is instead converted into an optimisation problem**. \n",
    "\n",
    "For example, we can ***define a scalar misfit function*** measuring in some sense the difference between model predictions and observations and seek to iteratively minimise this misfit. The least squares error is an example of this. We showed above that we could write down the general solution to this problem. But it's only feasible to use that approach for small cases. For larger problems we will need iterative solution methods such as in the tidal example above.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation - simple example\n",
    "\n",
    "Consider the problem: find $\\boldsymbol{x}\\equiv (x,y)$ which minimises the function\n",
    "\n",
    "$$ f(\\boldsymbol{x}) = 1+2x + 4y + x^2+2xy+3y^2$$\n",
    "\n",
    "\n",
    "The following image shows a contour plot of the function, and the red star indicates the $(x,y)$ location of the minima\n",
    "\n",
    "<img src=\"./figures/simple_optimisation.png\" width=300x>\n",
    "\n",
    "A homework exercise asks you to compute the minima and to generate this image.\n",
    "\n",
    "You'll cover methods to solve this type of problem efficiently for much more complex cases later in this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation (more complex)\n",
    "\n",
    "The following movie shows iterations from an optimisation problem where our task is to maximise the power generated by an array of 256 individual tidal turbines. At every iteration a shallow water solver computes the flow field and the power of the array. The design parameters are then the $(x,y)$ location of each of the turbines.\n",
    "\n",
    "For more information see: [Tidal turbine array optimisation using the adjoint approach, SW Funke, PE Farrell, MD Piggott, Renewable Energy 63, 658-673](https://www.sciencedirect.com/science/article/pii/S0960148113004989).\n",
    "\n",
    "<video controls loop width=\"600\" src=\"animations/array.mp4\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "You'll return to this example in a later lecture on \"PDE-constrained optimisation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "# More on forward and inverse problems  [*]\n",
    "\n",
    "## Abstract problem setting\n",
    "\n",
    "Building on the above motivation and various examples, let's think a bit more about the problem at hand.\n",
    "\n",
    "Our starting point is a model for a system that can be written as \n",
    "\n",
    "$$G(\\boldsymbol{m}) = \\boldsymbol{d}$$\n",
    "\n",
    "where $G$ is a function or operator that produces the output, or data,  vector $\\boldsymbol{d}$ for given input parameters $\\boldsymbol{m}$.  So $G$ can be thought of as a numerical model describing the underlying \"physics\", as well as things like the geometry and boundary conditions etc.\n",
    "\n",
    "Generation of the forward model above was covered in ACSE3 (e.g. finite difference, volume or element methods) and solving the above **forward model**: given $\\boldsymbol{m}$ compute $\\boldsymbol{d}$, may require linear or nonlinear solves (again ACSE3) so may be costly, but in general is straightforward once a stable and robust $G$ has been constructed.\n",
    "\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "The **inverse problem** asks the inverse model, for a given (e.g. set of observational data $\\boldsymbol{d}$) what can we conclude about the model parameters $\\boldsymbol{m}$, i.e. conceptually can we do the following:\n",
    "\n",
    "$$\\boldsymbol{m} = G^{-1}(\\boldsymbol{d}) $$\n",
    "\n",
    "where in general we won't be able to explicitly form the inverse operator $G^{-1}$.\n",
    "\n",
    "Additionally, it will often be the case that the inverse equation has no exact  solution  at  all  (**non-existence**),  or  that  there  will be infinitely many solutions that all fit the data equally accurately (**non-uniqueness**), or that the solution for the model parameter $\\boldsymbol{m}$ varies  significantly when only very small changes are made to  the input data $\\boldsymbol{d}$ (**instability**). \n",
    "\n",
    "We also need to contend with the fact that in the real world, observations always contain errors, as well as the fact that our representation of the physics $G$ will be imperfect, or our mesh resolution will be finite and some processes will be missing.  We therefore need to instead think about the relation\n",
    "\n",
    "$$\\boldsymbol{d} = G(\\boldsymbol{m}) + \\boldsymbol{e}$$\n",
    "\n",
    "where $\\boldsymbol{e}$  represents the errors in the observations and/or model from all contributions which we may be able to estimate but do not know explicitly.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "For linear problems, e.g. as may be formulated via the linearisation of a nonlinear problem, our forward model can be written as \n",
    "\n",
    "$$A\\, \\boldsymbol{x} = \\boldsymbol{b}$$\n",
    "\n",
    "where $A\\in\\mathbb{R}^{m\\times n}$ is a matrix.  \n",
    "\n",
    "\n",
    "We of course have seen this situation several times in this course already - with me in ACSE2 and ACSE3. \n",
    "\n",
    "<br>\n",
    "\n",
    "NB. we noted in those courses the issue of ***ill-conditioning*** - sometimes our matrix is of a form where small inevitable errors in $A$ or $\\boldsymbol{b}$ can be hugely amplified, and unless we are careful and use robust solution approaches our solution $\\boldsymbol{b}$ may be polluted beyond use with errors.\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that compared to previous modules e have a crucial new aspect now - ***$A$ will NOT typically be a square matrix*** - that is, the number of equations and unknowns  will  typically  not  be  the  same.    Even  when  $A$  is  square,  it  will  often  also  be singular so that the inverse problem will not often have an immediately straightforward solution.  Remember that for a singular square matrix we may have zero or infinitely many solutions depending on the exact nature of the data/RHS vector.\n",
    "\n",
    "The forward equation is sometimes called the equation of condition. The matrix $A$ that appears within it is variously called the sensitivity matrix, the condition matrix, the sensitivity kernel or the data kernel of the problem. \n",
    "\n",
    "In contrast, non-linear problems do not obey superposition and scalability, so that if the magnitude of the model parameters is varied, then the data will not simply be changed by an equivalent scaling in magnitude, and if two models  are combined by addition then the resultant data will not be a simple addition of the two original datasets.  Many important physical problems are non-linear, and in general the solution of non-linear problems is more difficult than the solution of linear problems. Fortunately it is often possible to solve non-linear problems by solving a sequence of linear  problems that approximate the non-linear problem. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "## Real world problems\n",
    "\n",
    "Numerical inversion then is the theory and practice of solving equations that can be represented by $G(\\boldsymbol{m}) = \\boldsymbol{d} $ or $A\\boldsymbol{x} =  \\boldsymbol{b} + \\boldsymbol{e}$, and that produces useful solutions efficiently, despite the problems of the non-existence, non-uniqueness, instability and data errors that will typically characterise these problems. \n",
    "\n",
    "Some key elements of real-world numerical inverse problems are: \n",
    "\n",
    "\n",
    "- Inverse problems are typically characterised by a large number of parameters that may be conceptually infinite, although in practice we will only ever be able to use a finite number of model parameters in order to be able to solve the forward problem computationally. \n",
    "\n",
    "\n",
    "- The number of observed data will always be finite. \n",
    "\n",
    "\n",
    "- Observed data always contain measurement errors and noise.\n",
    "\n",
    "\n",
    "- It will usually be impossible to estimate perfectly all the model parameters from data that are likely to be inaccurate, insufficient or inconsistent, but it is possible nonetheless to extract useful and sometimes powerful constraints about the values of these parameters and/or the relationships between them.\n",
    "\n",
    "\n",
    "- We will almost always have additional prior information about the plausibility of model parameters and their inter-relationships. \n",
    "\n",
    "\n",
    "- Most important problems will be non-linear.\n",
    "\n",
    "\n",
    "- The computational cost of the forward problem is likely to be significant so that trial-and-error methods are likely to be impractical.\n",
    "\n",
    "\n",
    "- Understanding and quantifying the uncertainty in the model parameters may be as important as estimating their values.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "    \n",
    "## Examples\n",
    "\n",
    "Two important examples of inversion problems (that are often cast as optimisation problems) are data assimilation and imaging.\n",
    "\n",
    "- [Data assimilation](https://en.wikipedia.org/wiki/Data_assimilation) (e.g. of the atmosphere in numerical weather prediction) seeks to combine a model with observational data for lots of different goals, e.g. to provide the best estimate for the initial condition of a weather forecast.\n",
    "\n",
    "\n",
    "- [Imaging](https://en.wikipedia.org/wiki/Inverse_problem) (e.g. medical or geophysical) seeks to reveal hidden internal details (of bodies or the Earth) given observational data of travel times of some sort of wave (e.g. ultrasound scans) that is transmitted through the body being imaged. The underlying model $G$ is then often a wave equation solver.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "# Linear algebra review [*]\n",
    "\n",
    "## Quick reminder on matrix notation & terminology\n",
    "\n",
    "So in this lecture we will be dealing with linear matrix systems, let's remind ourselves on some notation\n",
    "\n",
    "\n",
    "- We will use bold fonts to indicate vectors, e.g. $\\boldsymbol{x } $ :\n",
    "\n",
    "$$ \\boldsymbol{x} = (x_1,x_2,\\ldots, x_m)^T$$\n",
    "\n",
    "(the transpose is here since we generally assume a vector is a *column* vector (i.e. has dimension, or shape, $m\\times 1$), but it's clearly easier and more compact to display it as the transpose of a *row* vector in writing - note that when writing on paper it's common to indicate a bold symbol with an underline: $\\boldsymbol{x} \\equiv \\underline{x} $. Sometimes people will use $\\vec{x}$.)\n",
    "\n",
    "- and capital letters to indicate matrices, e.g. $A$:\n",
    "\n",
    "$$A = \n",
    "\\begin{pmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "\n",
    "- Subscript letters or numbers will be used to indicate components of arrays or matrices, e.g. $x_i$ is the $i$-th component of the vector $\\boldsymbol{x}$, and $a_{ij}$ is the entry from the $i$-th row and the $j$-th column of the matrix $A$, with of course us needing to be careful over our numbering which starts from zero in our Python implementations.\n",
    "\n",
    "\n",
    "- We will identify column vectors of the matrix $A$ as\n",
    "\n",
    "$$A = \\begin{pmatrix}\n",
    "      &     &         &     \\\\\n",
    "  \\boldsymbol{a}_{\\,:1} & \\boldsymbol{a}_{\\,:2} &  \\ldots & \\boldsymbol{a}_{\\,:n} \\\\\n",
    "      &     &         &     \n",
    "\\end{pmatrix}$$\n",
    "i.e. $\\boldsymbol{a}_{\\,:j}$ indicates the $j$-th column of matrix $A$.\n",
    "\n",
    "\n",
    "Note that I've made the choice to start my indices at '1' for the purposes of writing things down mathematically. \n",
    "You could choose to start with '0' to better match Python indexing if you want, you just need to be consistent.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "A general [matrix](https://en.wikipedia.org/wiki/Matrix_(mathematics)) of ***dimension***(shape in NumPy) $m \\times n$ is\n",
    "\n",
    "\\begin{pmatrix}\n",
    "    a_{11} & a_{12} & \\dots & a_{1n} \\\\\n",
    "    a_{21} & a_{22} & \\dots & a_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1} & a_{m2} & \\dots & a_{mn}   \n",
    "\\end{pmatrix}\n",
    "\n",
    "where for a real matrix all *entries* $a_{ij}\\in\\mathbb{R}$, i.e. are real numbers, and for a complex matrix $a_{ij}\\in\\mathbb{C}$.  \n",
    "\n",
    "Mathematically we could also write $A\\in\\mathbb{R}^{m\\times n}$ or $\\in\\mathbb{C}^{m\\times n}$.\n",
    "\n",
    "We would say out loud that the matrix is of dimension \"m rows by n columns\" or just \"m by n\".\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "## Linear systems\n",
    "\n",
    "\n",
    "Consider the problem written in the form\n",
    "\n",
    "$$A\\boldsymbol{x}=\\boldsymbol{b},$$\n",
    "\n",
    "where for a given $m\\times n$ matrix $A$ and a given $m\\times 1$ right hand side vector $\\boldsymbol{b}\\;$ we want to find an $n\\times 1$ vector $\\boldsymbol{x}\\;$.\n",
    "\n",
    "Let's start by considering the square problem, i.e. where $m=n$. \n",
    "\n",
    "This was the case we considered in the *Numerical Methods module* for example where we considered topics such as the use of Gaussian elimination to solve problems of this type.\n",
    "\n",
    "In this module we are particularly interested in the case where the matrix may not be square, i.e. where the vectors $\\boldsymbol{x}$ and $\\boldsymbol{b}$ are of different lengths.\n",
    "Mutiplication by $A$ can then be interpreted as mapping vectors or points from $\\mathbb{R}^n$ into $\\mathbb{R}^m$.\n",
    "\n",
    "\n",
    "In all situations it's important that we understand **whether our problem has a solution** and **whether that solution is unique**.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"optional\">\n",
    "\n",
    "### A simple square example  <a class=\"tocSkip\">\n",
    "\n",
    "Consider a simple example\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "  2x + 3y &=& 7 \\\\[5pt]\n",
    "   x - 4y &=& 3\n",
    "\\end{eqnarray*} \n",
    "\n",
    "this is completely equivalent to the following problem written in matrix form:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    2 & 3 \\\\\n",
    "    1 & -4  \n",
    "\\end{pmatrix}  \n",
    "\\begin{pmatrix}\n",
    "    x \\\\\n",
    "    y \n",
    "\\end{pmatrix}  =\n",
    "\\begin{pmatrix}\n",
    "    7 \\\\\n",
    "    3 \n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "We can rearrange the first of these equations to\n",
    "\n",
    "$$y = -\\frac{2}{3} x + \\frac{7}{3}$$\n",
    "\n",
    "which we recognise as the equation of a straight line in 2D space: $y=mx +c$ where $m$ is the slope and $c$ is the intercept.\n",
    "\n",
    "Similarly the second equation can be rearranged into the standard form for a straignt line in 2D.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "    \n",
    "## Expanding the geometrical thinking\n",
    "\n",
    "Let's think through another interpretation which further utilises geometrical thinking. \n",
    "\n",
    "Consider all of $(x,y)$ space where $x$ and $y$ are independent variables that are free to vary arbitrarily - they will map out, or cover/span, all of 2D space.  \n",
    "\n",
    "By stating that $ 2x + 3y = 7 $ we are now saying that $x$ and $y$ are not both independent - they cannot vary arbitrarily, one is effectively dependent on the other. \n",
    "\n",
    "For a linear system this one equation *restricts*, or *constrains*, us to a line in 2D (with a linear equation containing $x$, $y$ and $z$ variables constraining us to a plane in 3D etc). \n",
    "\n",
    "Indeed we can think about the equation as a constraint, we might even simply call it a constraint rather than an equation.\n",
    "\n",
    "For nonlinear problems we can still think of the model as a mapping, and still consider this in the form of a series of constraints over what potential output values our input data gets mapped to.\n",
    "\n",
    "Above we wrote this as $y\\equiv y(x)$ ($y$ is a function of $x$) to emphasise a point, but of course we could have just as easily rearranged to $x\\equiv x(y)$. If instead of $2x + 3y = 7$ we had the equation  $2x = 7$ then of course we couldn't rearrange into the form $y\\equiv y(x)$, but trivially can into the form $x\\equiv x(y)$ (a constant in this case) - this just reflects the fact that this particular equation example represents a vertical line in 2D space ($m=\\infty$)!\n",
    "\n",
    "Of course our second equation can also be interpreted as a line in 2D space.\n",
    "\n",
    "Let's go back to our original example and plot the two lines that are defined by our two equations/constraints.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,5,100)\n",
    "y1 = -(2./3.)*x + (7./3.)\n",
    "y2 = (1./4.)*x - (3./4.)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.set_xlabel(\"$x$\", fontsize=14)\n",
    "ax1.set_ylabel(\"$y$\", fontsize=14)\n",
    "ax1.grid(True)\n",
    "\n",
    "ax1.plot(x,y1,'b', label='$2x+3y=7$')\n",
    "ax1.plot(x,y2,'r', label='$x-4y=3$')\n",
    "\n",
    "ax1.legend(loc='best', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "The blue line maps out all combinations of $x$ and $y$ values that satisfy the first equation/constraint, and the red all the $x$ and $y$ values that satisfy the second.\n",
    "\n",
    "So in solving the system of simultaneous equations, or equivalently solving the corresponding matrix system, we are asking the question what $x$ and $y$ values satisfy BOTH equations (or constraints)?\n",
    "\n",
    "\n",
    "From the image above we see that **there is a solution** (at the intersection) and it is clearly **unique** (there is only one intersection), for these particular equations. This is the only combination of $x$ and $y$ values that satisfy BOTH equations/constraints.\n",
    "\n",
    "\n",
    "In 2D for this example, one constraint restricts our 2D space of $x,y$ values to a 1D subset, the other to another 1D subset, combined they restrict the 2D space down to 0D, i.e. a single point - our unique solution.\n",
    "\n",
    "\n",
    "But of course in 2D there are two other possible scenarios - what are these?  Can you construct suitable pairs of equations that demonstrate these possibilities, and the corresponding plots to help explain what is going in visually? \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "We saw previously that for this simple system we can either use substitution, Gaussian elimination (which effectively just formalises substitution), the construction of the inverse matrix, or make use of a linear solver, to fine the solution to this problem\n",
    "\n",
    "$$x = \\frac{37}{11}, \\quad  y = \\frac{1}{11} $$\n",
    "\n",
    "We can check this solution by adding this point to our plot and comfirm if it agrees with the intersection.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,5,100)\n",
    "y1 = -(2./3.)*x + (7./3.)\n",
    "y2 = (1./4.)*x - (3./4.)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.set_xlabel(\"$x$\", fontsize=14)\n",
    "ax1.set_ylabel(\"$y$\", fontsize=14)\n",
    "ax1.grid(True)\n",
    "\n",
    "ax1.plot(x,y1,'b', label='$2x+3y=7$')\n",
    "ax1.plot(x,y2,'r', label='$x-4y=3$')\n",
    "ax1.plot(37./11,1./11, 'ko', label='Our solution')\n",
    "\n",
    "ax1.legend(loc='best', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "## Solvability of linear systems\n",
    "\n",
    "If $\\det(A)=0$ ($A$ is singular) a linear system featuring the matrix $A$ with any RHS does **not** have a **unique** solution, it **may have either infinite *or* no solutions**.\n",
    "\n",
    "For example, consider\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "  \\begin{array}{rr}\n",
    "    2 & 3 \\\\\n",
    "    4 & 6 \n",
    "  \\end{array}\n",
    "\\right)\\left(\n",
    "  \\begin{array}{c}\n",
    "    x \\\\\n",
    "    y\n",
    "  \\end{array}\n",
    "\\right) = \\left(\n",
    "  \\begin{array}{c}\n",
    "    4 \\\\\n",
    "    8\n",
    "  \\end{array}\n",
    "\\right),\n",
    "$$\n",
    "\n",
    "where the matrix on the LHS clearly has a zero determinant.\n",
    "\n",
    "Considering now the values in the RHS vector as well, the second equation is simply twice the first, and hence a solution to the first equation is also automatically a solution to the second equation.  In this case as the equations do not contradict one another we say that the equations are **consistent**.\n",
    "\n",
    "If we think geometrically, and interpret the two equations as constraints, they are **both** constraining our $x,y$ values to the same 1D subspace - any solution along this line in 2D satisfies **both** equations, and hence any of them is a solution to our linear system.\n",
    "\n",
    "We hence only have one *linearly-independent* equation here, and our problem is under-constrained: we effectively only have one equation for two unknowns and this problem has *infinitely many* possibly solutions (or said another way, we have *existence*, but *non-uniqueness*): e.g. $\\boldsymbol{x}=(2,0)^T$ is a solution, so is $\\boldsymbol{x}=(-1,2)^T$, etc.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "If we instead replaced the RHS vector with $(4,7)^T$, then the two equations would now be contradictory: in this case we have *no solutions* (or *non-existence*).\n",
    "\n",
    "    \n",
    "<br><br>\n",
    "    \n",
    "Note that this case is revisited in the homework and in the supplementary notebook that includes more visualisations.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "### Linear (in)dependence\n",
    "\n",
    "Note that a set of vectors where one can be written as a linear sum of the others are termed [***linearly-dependent***](https://en.wikipedia.org/wiki/Linear_independence). \n",
    "\n",
    "(As was the case for the rows and the columns in the example immediately above - the first row of $A$ is simply twice the second; the second column is $3/2$ times the first).\n",
    "\n",
    "<br>\n",
    "\n",
    "When this is **not** the case the vectors are termed [***linearly-independent***](https://en.wikipedia.org/wiki/Linear_independence).\n",
    "\n",
    "<br>\n",
    "\n",
    "When the rows are linearly dependent what can you conclude about the expected structure of the row echelen form of the matrix? I.e. what do you expect to end up if you add and subtract multiples of rows to other rows in order to simplify the matrix? ............ [it's possible for us to obtain a row made up of all zeros!]\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "### A basis for a vector space\n",
    "\n",
    "A <a href=\"https://en.wikipedia.org/wiki/Basis_(linear_algebra)\">*basis*</a> for a vector (or linear) space is a set of *linearly-independent* vectors such that every vector (or location) in the vector space is a linear combination of this set.\n",
    "\n",
    "<br>\n",
    "\n",
    "[This a [goldilocks](https://en.wikipedia.org/wiki/Goldilocks_and_the_Three_Bears) like definition - we have exactly the right number of vectors - too few and we can't reach every point in the space through their combination, and too many some will be redundant and the set of vectors will be linearly dependent.]\n",
    "\n",
    "<br>\n",
    "\n",
    "The number of vectors in this set is called the **dimension** of the vector space.\n",
    "\n",
    "For example, the unit vectors $\\,\\boldsymbol{i}$, $\\boldsymbol{j}$, $\\boldsymbol{k}\\,$ are linearly-independent and form a basis for the [three-dimensional space $\\mathbb{R}^3$](https://en.wikipedia.org/wiki/Three-dimensional_space). These unit vectors are defined as\n",
    "\n",
    "$$\\boldsymbol{i} = \\left(\n",
    "  \\begin{array}{c}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "    0     \n",
    "  \\end{array}\n",
    "\\right),\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\n",
    "\\boldsymbol{j} = \\left(\n",
    "  \\begin{array}{c}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "    0     \n",
    "  \\end{array}\n",
    "\\right),\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\n",
    "\\boldsymbol{k} = \\left(\n",
    "  \\begin{array}{c}\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    1     \n",
    "  \\end{array}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "By definition, these three vectors **forming a basis means that we can reach any point in $\\mathbb{R}^3$ through summing appropriate multiples of them**.\n",
    "\n",
    "These are the basis vectors which we commonly choose (as they're in some sense the simplest and cleanest choice), but clearly they are not unique - with any linearly independent set of three vectors we can reach any point in 3D space through their weighted summation - start thinking about these basis vectors being columns of a matrix forming a linear system!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "### An example | the geometrical interpretation of matrix vector multiplication\n",
    "\n",
    "Consider our $2\\times 2$ example from earlier\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  2x + 3y &= 7 \\\\[5pt]\n",
    "   x - 4y &= 3 \n",
    "\\end{align*}\n",
    "   \\quad \\iff \\quad\n",
    "  \\begin{pmatrix}\n",
    "    2 & 3 \\\\\n",
    "    1 & -4  \n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    x \\\\\n",
    "    y \n",
    "  \\end{pmatrix}=\n",
    "  \\begin{pmatrix}\n",
    "    7 \\\\\n",
    "    3 \n",
    "  \\end{pmatrix}   \n",
    "$$\n",
    "\n",
    "If we define two vectors given by the columns of the matrix, which we plot below in blue and red, \n",
    "\n",
    "along with the point $\\boldsymbol{b}$ we want to arrive at,\n",
    "\n",
    "\n",
    "we can then visualise the problem of solving the linear system as wanting to reach the black point through the appropriate weighted sum of the blue and red vectors.  Why is this?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.set_xlabel(\"$x$\", fontsize=14)\n",
    "ax1.set_ylabel(\"$y$\", fontsize=14)\n",
    "ax1.set_title('Position as a sum over matrix cols', fontsize=14)\n",
    "#ax1.grid(True)\n",
    "\n",
    "i = np.array([1,0])\n",
    "j = np.array([0,1])\n",
    "\n",
    "# plot the unit vectors\n",
    "ax1.quiver(i[0], i[1], angles='xy', scale_units='xy', scale=1, color='k')\n",
    "ax1.quiver(j[0], j[1], angles='xy', scale_units='xy', scale=1, color='k')\n",
    "\n",
    "ax1.set_xlim(-1,8)\n",
    "ax1.set_ylim(-5,5)\n",
    "\n",
    "A = np.array([[2, 3], [1, -4]])\n",
    "vec1 = A[:,0] # NB. This is the same as A*x\n",
    "vec2 = A[:,1] # NB. This is the same as A*y\n",
    "\n",
    "# plot them\n",
    "ax1.quiver(vec1[0], vec1[1], angles='xy', scale_units='xy', scale=1, color='b', width=0.02,  label='vec1')\n",
    "ax1.quiver(vec2[0], vec2[1], angles='xy', scale_units='xy', scale=1, color='r', width=0.02,  label='vec2')\n",
    "\n",
    "# add point b\n",
    "b = np.array([7., 3.])\n",
    "ax1.plot(b[0], b[1], 'ko',  label='${b}$')\n",
    "\n",
    "ax1.legend(loc='lower left', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "As already stated, we know trivially how to write the position $\\boldsymbol{b}$ as a sum over the unit vectors $\\boldsymbol{i}$ and $\\boldsymbol{j}$ (the black vectors indicate the unit vectors $\\boldsymbol{i}$ and $\\boldsymbol{j}$ in the plot above):\n",
    "\n",
    "$$\\boldsymbol{b} = \n",
    "\\begin{pmatrix}\n",
    "b_1\\\\\n",
    "b_2\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "b_1\\\\\n",
    "0\n",
    "\\end{pmatrix} +\n",
    "\\begin{pmatrix}\n",
    "0\\\\\n",
    "b_2\n",
    "\\end{pmatrix} =\n",
    "b_1\\begin{pmatrix}\n",
    "1\\\\\n",
    "0\n",
    "\\end{pmatrix} + b_2\n",
    "\\begin{pmatrix}\n",
    "0\\\\\n",
    "1\n",
    "\\end{pmatrix} =\n",
    "b_1 \\boldsymbol{i} + b_2\\boldsymbol{j}\n",
    "$$\n",
    "\n",
    "Finding the solution to our linear system is equivalent to asking how far along direction given by column one (blue vector), and then how far along the direction given by column two (the red vector), do I need to go to arrive at point $\\boldsymbol{b}$.\n",
    "\n",
    "This is exactly what the following interpretation of matrix-vector multiplication is telling us\n",
    "\n",
    "$$ A \\boldsymbol{x}=\\boldsymbol{b} \\quad \\iff \\quad\n",
    "\\begin{pmatrix}\n",
    "    b_1\\\\\n",
    "    b_2\\\\\n",
    "    \\vdots\\\\\n",
    "    b_m\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "      &     &         &     \\\\\n",
    "      &     &         &     \\\\      \n",
    "  \\boldsymbol{a}_{\\,:1} & \\boldsymbol{a}_{\\,:2} &  \\ldots & \\boldsymbol{a}_{\\,:n} \\\\\n",
    "      &     &         &     \\\\\n",
    "      &     &         &     \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "    x_1\\\\\n",
    "    x_2\\\\\n",
    "    \\vdots\\\\\n",
    "    x_n\n",
    "\\end{pmatrix}\n",
    "=\n",
    "x_1 \n",
    "\\begin{pmatrix}\n",
    "       \\\\\n",
    "       \\\\\n",
    "   \\boldsymbol{a}_{\\,:1} \\\\\n",
    "       \\\\\n",
    "    ~   \n",
    "\\end{pmatrix}\n",
    "+\n",
    "x_2 \n",
    "\\begin{pmatrix}\n",
    "       \\\\\n",
    "       \\\\\n",
    "   \\boldsymbol{a}_{\\,:2} \\\\\n",
    "       \\\\\n",
    "    ~   \n",
    "\\end{pmatrix}\n",
    "+ \\cdots +\n",
    "x_n\n",
    "\\begin{pmatrix}\n",
    "       \\\\\n",
    "       \\\\\n",
    "   \\boldsymbol{a}_{\\,:n} \\\\\n",
    "       \\\\\n",
    "    ~   \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "or in our case\n",
    "\n",
    "$$ \n",
    "\\begin{pmatrix}\n",
    "    7\\\\\n",
    "    3\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "      2 & 3   \\\\\n",
    "     1  & -4    \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "    x_1\\\\\n",
    "    x_2\n",
    "\\end{pmatrix}\n",
    "=\n",
    "x_1 \n",
    "\\begin{pmatrix}\n",
    "       2 \\\\\n",
    "       1\n",
    "\\end{pmatrix}\n",
    "+\n",
    "x_2 \n",
    "\\begin{pmatrix}\n",
    "     3 \\\\\n",
    "     -4\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "We can use the solution $\\boldsymbol{x}$ we have already found (`x = np.array([37./11, 1./11])`) to emphasise this visually/geometrically: \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.set_xlabel(\"$x$\", fontsize=14)\n",
    "ax1.set_ylabel(\"$y$\", fontsize=14)\n",
    "ax1.set_title('Position as a sum over matrix cols', fontsize=14)\n",
    "#ax1.grid(True)\n",
    "\n",
    "i = np.array([1,0])\n",
    "j = np.array([0,1])\n",
    "\n",
    "# plot the unit vectors\n",
    "ax1.quiver(i[0], i[1], angles='xy', scale_units='xy', scale=1, color='k')\n",
    "ax1.quiver(j[0], j[1], angles='xy', scale_units='xy', scale=1, color='k')\n",
    "\n",
    "\n",
    "ax1.set_xlim(-1,8)\n",
    "ax1.set_ylim(-5,5)\n",
    "\n",
    "A = np.array([[2, 3], [1, -4]])\n",
    "vec1 = A[:,0] # NB. This is the same as A*x\n",
    "vec2 = A[:,1] # NB. This is the same as A*y\n",
    "\n",
    "# plot them\n",
    "ax1.quiver(vec1[0], vec1[1], angles='xy', scale_units='xy', scale=1, color='b', width=0.02,  label='vec1')\n",
    "ax1.quiver(vec2[0], vec2[1], angles='xy', scale_units='xy', scale=1, color='r', width=0.02,  label='vec2')\n",
    "\n",
    "# add point b\n",
    "b = np.array([7., 3.])\n",
    "ax1.plot(b[0], b[1], 'ko',  label='${b}$')\n",
    "\n",
    "ax1.legend(loc='lower left', fontsize=14)\n",
    "\n",
    "# Now move x_1 in the direction given by column one, followed by x_2 in the direction given by column 2\n",
    "# and add to the plot with thinner lines\n",
    "x = np.array([37./11, 1./11])\n",
    "ax1.quiver(x[0]*vec1[0], x[0]*vec1[1], angles='xy', scale_units='xy', scale=1, color='darkblue', width=0.005,  label='vec1')\n",
    "ax1.quiver(x[0]*vec1[0], x[0]*vec1[1],x[1]*vec2[0], x[1]*vec2[1], angles='xy', \n",
    "           scale_units='xy', scale=1, color='darkred', width=0.005,  label='vec2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "So we need to take 37/11 multiplied by the blue arrow and add 1/11 multiplied by the red arrow to reach the black dot.\n",
    "\n",
    "[Note we don't say something like \"move 37/11 in the direction of the blue arrow, as we might with i,j,k, since these vectors are not normalised\"].\n",
    "\n",
    "Note that the columns are not orthogonal in this case, which we can demonstrate by:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vec1.dot(vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "if they were orthogonal the answer should have been zero,\n",
    "\n",
    "but that's fine as we can still reach any position in 2D space from their weighted sum as long as they are linearly independent, i.e. in the 2D case as long as they don't point in the same direction.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "## Solvability theory - some equivalent properties\n",
    "\n",
    "\n",
    "The following properties of a square $n\\times n$ matrix are equivalent:\n",
    "\n",
    "\n",
    "\n",
    "* $\\det(A)\\ne 0$, i.e. $A$ is non-singular.\n",
    "\n",
    "\n",
    "\n",
    "* The columns of $A$ are linearly independent.\n",
    "\n",
    "\n",
    "* The rows of $A$ are linearly independent.\n",
    "\n",
    "\n",
    "* The columns of $A$ [*span*](https://en.wikipedia.org/wiki/Linear_span) an $n$-dimensional space (this means that we can reach any point in $\\mathbb{R}^n$ through a linear combination of these vectors - recall we saw previously that this is simply what the operation $A\\boldsymbol{x}$ is doing if you write it out - the entries of $\\boldsymbol{x}$ are the weights in the linear combination - we wrote this out explicitly above.)  \n",
    "\n",
    "\n",
    "* [what this means is that for **any** given point in our space (described for example by a given $\\boldsymbol{b}$), we can reach that point through the appropriate summation of multiples of the columns of $A$, and its the $\\boldsymbol{x}$ solution vector that gives us these multiples.]\n",
    "\n",
    "\n",
    "* $A$ is invertible, i.e. there exists a matrix $A^{-1}$ such that $A^{-1}A = A A^{-1}=I$.\n",
    "\n",
    "\n",
    "* The matrix system $A\\boldsymbol{x}=\\boldsymbol{b}$ has a unique solution for every vector $\\boldsymbol{b}$.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "For even more equivalent properties for square matrices see <https://en.wikipedia.org/wiki/Invertible_matrix#The_invertible_matrix_theorem>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank, range and null-space\n",
    "\n",
    "Above we wrote for a square $n\\times n$ matrix, that being non-singular (and hence the linear system having a unique solution for any RHS vector) is equivalent to\n",
    "\n",
    "<br>\n",
    "\n",
    "* The columns of $A$ [*span*](https://en.wikipedia.org/wiki/Linear_span) an $n$-dimensional space (this means that we can reach any point in $\\mathbb{R}^n$ through a linear combination of these vectors - note that this is simply what the operation $A\\boldsymbol{x}$ is doing of course if you write it out - the entries of $\\boldsymbol{x}$ are the weights in the linear combination!)  \n",
    "\n",
    "<br>\n",
    "\n",
    "The columns of $A$ will always span an $r$-dimensional space, for some $r$, and assuming the matrix isn't the zero matrix $r \\ge 1$. \n",
    "\n",
    "What we're saying in the statement above is that *if the columns of $A$ span an $n$-dimensional space* (i.e. if $r=n$) then ... all the other properties in the previous cell hold.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Rank and range for square systems\n",
    "\n",
    "Let's explain a bit better what this means.\n",
    "\n",
    "The ***rank*** of an $n\\times n$ matrix $A$, written $\\text{rank}(A)$, is equal to the number of linearly independent columns that form it. It is also equal to the number of linearly independent rows that form it (as these two quantities, the column rank and the row rank can be proved to be equal). \n",
    "\n",
    "The matrix $A$ is said to have ***full rank*** if $\\text{rank}(A)=n$ and is said to be ***rank deficient*** if $\\text{rank}(A)<n)$ (in the square case - see below for the non-square case).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "The ***range*** (sometimes the ***image***) of an $n\\times n$ matrix $A$, written $\\text{range}(A)$, is the *space spanned by the columns* of $A$ (so it's also sometimes also called the [***column space***](https://en.wikipedia.org/wiki/Row_and_column_spaces)), i.e. every point in that space can be reached through a linear combination of the columns of $A$, equivalently every point in the space can be written as $A\\boldsymbol{x}$ for some $\\boldsymbol{x}$. Said another way, the columns of $A$ provide a ***basis*** for $\\text{range}(A)$.\n",
    "\n",
    "This is telling us that if our matrix has full rank then we can reach ***any*** location in $\\mathbb{R}^n$ through a weighted sum of the columns, i.e. no matter what the RHS vector is (the point we want to reach after we multiply by $A$) the corresponding linear system always has a unique solution and hence the matrix has an inverse. \n",
    "\n",
    "<br>\n",
    "\n",
    "If the matrix does not have full rank (i.e. $r<n$), that means we cannot reach all point in $\\mathbb{R}^n$ through a weighted sum of the columns. \n",
    "\n",
    "We will have (**non-unique**) solutions as long as the RHS vector (the location we want to get to geometrically) is in the range of $A$, as we are able to reach some points (i.e. some RHS vectors) with a weighted sum of our columns. \n",
    "\n",
    "If our given RHS vector is not in the range of $A$ then we simply cannot reach it through the weighted summation of columns (this is the **non-existence** case).\n",
    "\n",
    "Note that the difference between the last two points is completely down to the RHS vector and is nothing to do with the entries of the matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank and range in the non-square case\n",
    "\n",
    "Note that much of this generalises to the **non-square** case.\n",
    "\n",
    "<br>\n",
    "\n",
    "When $A$ is of size $m\\times n$ ($A \\in \\mathbb{R}^{m\\times n}$) we now think of $A$ as mapping vectors in $\\mathbb{R}^n$ into $\\mathbb{R}^m$ - matrix-multiplication can be interpreted as moving in a linear combination of directions given by the columns of the matrix in the space $\\mathbb{R}^m$. \n",
    "\n",
    "\n",
    "In the ***non-square*** case $A$ is said to have ***full rank*** if $\\text{rank}(A)=\\min(m,n)$ and is said to be ***rank deficient*** if $\\text{rank}(A)<\\min(m,n)$.  \n",
    "\n",
    "Note that as the column and row rank are also equal for non-square matrices, i.e. the number of linearly independent rows is the same as the number of linearly independent columns, then of course the maximum the rank of the matrix can ever be is $\\min(m,n)$.\n",
    " \n",
    "<br>\n",
    "\n",
    "Assuming $m\\ne n$ then there are two possibilities:\n",
    "\n",
    "\n",
    "1. In the \"tall\" [***over-determined***](https://en.wikipedia.org/wiki/Overdetermined_system) case ($m>n$), the range (or column space) of $A$ doesn't (cannot) cover all of the space that our data $\\boldsymbol{b}$ lives in. But if $\\boldsymbol{b}$ does live in the range then we can find a solution (it may or may not be unique).\n",
    "\n",
    "\n",
    "2. In the \"fat\" [***under-determined***](https://en.wikipedia.org/wiki/Underdetermined_system) case ($m<n$) it's the opposite, if the rank of $A$ is equal to the dimension of the data space (as will be the case if the matrix is full rank) then there will always be multiple linear combinations of the columns that get us to any point in the vector space containing the data, i.e. we will always have a non-unique solution for all RHS $\\boldsymbol{b}$ vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The null-space\n",
    "\n",
    "The ***null-space*** of the matrix $A$, written $\\text{null}(A)$, is the set of vectors $\\boldsymbol{x}$ that satisfy $A\\boldsymbol{x} = 0$.\n",
    "\n",
    "Why is the null-space important? Well for lots of reasons including: \n",
    "\n",
    "\n",
    "1. this is how we find eigenvectors, \n",
    "    \n",
    "    \n",
    "    \n",
    "2. when solving a linear system $A\\boldsymbol{x}=\\boldsymbol{b}$, we can add any vector (or linear combination of vectors) from the null-space to a solution and get another solution, this will demonstrate non-uniqueness of solutions for this problem if the null-space contains anything other than the zero vector. \n",
    "\n",
    "\n",
    "Note that the null-space always contains the zero vector, we are not really interested in that solution, it's non-zero vectors that are important. Note if we have a non-zero vector in the null-space, then any scalar multiple of that vector will also be in the null space ($A\\boldsymbol{x}=\\boldsymbol{0}\\Rightarrow A(\\alpha\\boldsymbol{x})=\\alpha A \\boldsymbol{x}= \\boldsymbol{0}$).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## The rank-nullity theorem\n",
    "\n",
    "A fundamental result for any matrix $A \\in \\mathbb{R}^{m\\times n}$, which maps vectors from $\\mathbb{R}^{n}$ into $\\mathbb{R}^{m}$, is the so-called [rank-nullity theorem](https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem).\n",
    "\n",
    "This states that the dimension of the span of the columns of $A$ (i.e. its rank) and the dimension of its null space (termed the nullity) must sum to $n$ - the number of columns:\n",
    "\n",
    "$$ \\text{rank}(A) + \\dim(\\text{null}(A)) = n$$\n",
    "\n",
    "This means that if you have worked out the rank you automatically know the dimension of the null space (and vice-versa), in particular if it contains any non-zero vectors $\\boldsymbol{x}$ such that $A\\boldsymbol{x}=\\boldsymbol{0}$.\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that this will always be the case for under-determined systems where $m<n$ as the rank can be no larger than  $\\min(m,n)$.\n",
    "\n",
    "It may be the case for over-determined systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Reduced) row echelon form of a matrix and links to the matrix rank and null space\n",
    "\n",
    "\n",
    "Recall that row operations performed on the augmented matrix formed of the LHS matrix and the RHS vector transform the system into a new system that has the same solution. The point is that we do this so that the new form is trivial to solve, e.g. using back substitution in the case of Gaussian eliminaton.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Recall that **row operations** are defined as\n",
    "\n",
    "1. multiply each row by a non-zero scalar\n",
    "\n",
    "\n",
    "2. add multiples of one row to another \n",
    "\n",
    "\n",
    "3. swap rows\n",
    "\n",
    "<br>\n",
    "\n",
    "It's *also* the case that row operations do not change the rank of a matrix. Therefore one way to establish the rank of a matrix is to transform it into (R)REF and read off the rank of the transformed matrix as the number of linearly independent rows (equivalently the number of linearly independent columns), and this is also the rank of the matrix in its original form.  \n",
    "\n",
    "\n",
    "Note that sometimes we will want to compute the rank of a matrix $A$, but sometimes the rank of the augmented matrix $[A|\\boldsymbol{b}]$.\n",
    "\n",
    "    \n",
    "<br> \n",
    "\n",
    "Recall that a matrix is in [***row echelon form***](https://en.wikipedia.org/wiki/Row_echelon_form) when\n",
    "\n",
    "\n",
    "- all non-zero rows are above any rows of all zeros (NB. if we do have rows of all zeros then the matrix is singular), and\n",
    "\n",
    "\n",
    "- the leading coefficient in the row (first non-zero entry from the left) is always strictly to the right of the leading coefficient of the row above\n",
    "\n",
    "\n",
    "<br>\n",
    "If in addition\n",
    "\n",
    "- the leading coefficient in any non-zero row is unity and every column containing a leading '1' has zeros everywhere else within that column, we say the matrix is in ***reduced row echelon form***.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "#### Problems with a zero RHS  <a class=\"tocSkip\">\n",
    "\n",
    "\n",
    "Question:  what happens to the system undergoing row operations in the case when $\\boldsymbol{b} = \\boldsymbol{0}$?\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Suppose you do row operations to a system with a zero RHS ($A\\boldsymbol{x}=\\boldsymbol{0}$) and are able to end up with this system\n",
    "\n",
    "$$\n",
    "  \\begin{pmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1  \n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    x \\\\\n",
    "    y \n",
    "  \\end{pmatrix}=\n",
    "  \\begin{pmatrix}\n",
    "    0 \\\\\n",
    "    0 \n",
    "  \\end{pmatrix}   \n",
    "$$\n",
    "\n",
    "what does this tell you about possible solutions to the problem $A\\boldsymbol{x}=\\boldsymbol{0}$?\n",
    "\n",
    "<br>\n",
    "\n",
    "Now suppose instead of the final form above you end up with the following\n",
    "\n",
    "$$\n",
    "  \\begin{pmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 0  \n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    x \\\\\n",
    "    y \n",
    "  \\end{pmatrix}=\n",
    "  \\begin{pmatrix}\n",
    "    0 \\\\\n",
    "    0 \n",
    "  \\end{pmatrix}   \n",
    "$$\n",
    "\n",
    "what does this tell you about possible solutions to the problem $A\\boldsymbol{x}=\\boldsymbol{0}$?\n",
    "\n",
    "<br>  \n",
    "\n",
    "Note that for a matrix $A$ the space $\\{\\boldsymbol{x} | A\\boldsymbol{x} = \\boldsymbol{0}\\} $ is called the null space of $A$. \n",
    "\n",
    "So note that as well as solving linear systems, row operations give us a method for calculating the null space of a matrix (for this case you do not need to both including the RHS in the augmented matrix, as we know it can never change from the zero vector under row operations), and the dimension of the null space helps us define the rank of a matrix (see later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the rank and the null space\n",
    "\n",
    "How can we compute the rank and the null-space for a given matrix?\n",
    "\n",
    "We can do this using row operations.  \n",
    "\n",
    "We already stated that row operations, when performed on the augmented matrix, results in an updated linear system with the same solution. This is done of course to construct an updated augmented system which corresponds to a system that is trivial to solve, this solution giving the solution to the original non-trivial problem.\n",
    "\n",
    "It's also the case that performing row operations (on a matrix itself or the augmented system) does not change the rank of a matrix (or the augmented system).\n",
    "\n",
    "Hence, we can compute the rank of a matrix by performing row operations on it to transform it to the simpler (reduced) row echelon form.\n",
    "\n",
    "We can then easily read off the rank of the transformed matrix as the number of linearly independent rows (equivalently the number of linearly independent columns), and this is also the rank of the matrix in its original form.\n",
    "\n",
    "See a homework exercise, and\n",
    "<https://www.statisticshowto.datasciencecentral.com/reduced-row-echelon-form-2/>.\n",
    "\n",
    "<br>\n",
    "\n",
    "####  A worked example  <a class=\"tocSkip\">\n",
    "\n",
    "\n",
    "For example consider the following square matrix case\n",
    "\n",
    "$$ A =\n",
    "\\begin{pmatrix}\n",
    "-2 & -4 & -20 \\\\\n",
    "2 & 6 & 24 \\\\\n",
    "2 & 10 & 32\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Let's multiply the first row by -1/2 in order to generate a one in the first entry of the first row (we could choose to swap rows first, so of course there is no unique way to go about this process), this gives us\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 10 \\\\\n",
    "2 & 6 & 24 \\\\\n",
    "2 & 10 & 32\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Remove the entries below that \"1\" that's now in the top left by subtracting multiples of the first row from the second and third:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 10 \\\\\n",
    "0 & 2 & 4 \\\\\n",
    "0 & 6 & 12\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Now turn the first entry in the second row into a \"1\"\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 10 \\\\\n",
    "0 & 1 & 2 \\\\\n",
    "0 & 6 & 12\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and remove the value below, as well as the value above, by subtracting the appropriate multiples of the second row from the first and the third:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 6 \\\\\n",
    "0 & 1 & 2 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "You can hopefully see that this has two linearly independent columns (we can easily see how the first two columns can be combined to form the third), \n",
    "\n",
    "it also has two linearly independent rows (as we expect). \n",
    "\n",
    "The rank of the matrix in this final form is thus 2, and therefore  $\\text{rank}(A)=2$.\n",
    "\n",
    "This means that the dimension of the range of $A$ is two, i.e. the range will span a plane within $\\mathbb{R}^m = \\mathbb{R}^3$.\n",
    "\n",
    "\n",
    "We can check this using `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the rank of the matrix in original form\n",
    "A = np.array([[-2, -4, -20], [2 , 6 , 24], [2, 10, 32]])\n",
    "pprint(A)\n",
    "print(np.linalg.matrix_rank(A))\n",
    "\n",
    "# compute the rank of the matrix in a form after we've performed row operations\n",
    "Arref = np.array([[1, 0, 6], [0 , 1 , 2], [0, 0, 0]])\n",
    "pprint(Arref)\n",
    "print(np.linalg.matrix_rank(Arref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a series of points in 3D space (equivalently vectors extending from the origin) and see how they transform under multiplication of $A$. If above is all correct the dimension of the mapped points/vectors should be 2 - it should be a 2D plane in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the transformation\n",
    "\n",
    "A = np.array([[-2, -4, -20], [2 , 6 , 24], [2, 10, 32]])\n",
    "\n",
    "# construct some points in 3D space\n",
    "x = np.linspace(-1, 1, 5)\n",
    "y = np.linspace(-1, 1, 5)\n",
    "z = np.linspace(-1, 1, 5)\n",
    "# this creates a mesh of points in 2D\n",
    "xx, yy, zz = np.meshgrid(x, y, z)\n",
    "# convert to row vectors\n",
    "xxx = np.reshape(xx,(1,np.size(xx)))\n",
    "yyy = np.reshape(yy,(1,np.size(yy)))\n",
    "zzz = np.reshape(zz,(1,np.size(zz)))\n",
    "# convert to a 3 x N matrix of vectors/points\n",
    "vecs = np.vstack((xxx,yyy,zzz))\n",
    "\n",
    "# transform these points\n",
    "Avecs = A@vecs\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax1.plot(vecs[0,:], vecs[1,:], vecs[2,:], 'r.')\n",
    "ax1.set_xlabel('$x$', fontsize = 16)\n",
    "ax1.set_ylabel('$y$', fontsize = 16)\n",
    "ax1.set_zlabel('$z$', fontsize = 16)\n",
    "ax1.set_title('$\\mathbb{R}^n$', fontsize = 16)\n",
    "ax1.set_xlim3d(-1, 1)\n",
    "ax1.set_ylim3d(-1, 1)\n",
    "ax1.set_zlim3d(-1, 1)\n",
    "                      \n",
    "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax2.plot(Avecs[0,:], Avecs[1,:], Avecs[2,:], 'r.')\n",
    "ax2.set_xlabel('$x$', fontsize = 16)\n",
    "ax2.set_ylabel('$y$', fontsize = 16)\n",
    "ax2.set_zlabel('$z$', fontsize = 16)\n",
    "ax2.set_title('$\\mathbb{R}^m$', fontsize = 16)\n",
    "ax2.set_xlim3d(-20, 20)\n",
    "ax2.set_ylim3d(-20, 20)\n",
    "ax2.set_zlim3d(-20, 20)\n",
    "\n",
    "# rotate to try and get a better view - with a different plotting backend you could rotate with mouse\n",
    "# you could edit this to try and get a better idea of the 3D view\n",
    "ax2.view_init(20, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB. see the supplementary notebook within which you will be able to rotate 3D axes to get a better feel for things like 2D planes embedded in 3D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we can compute the null space. \n",
    "\n",
    "As row operations will never change the zero vector, we don't actually need to perform row operations on the augmented matrix $[A|\\boldsymbol{b}]$ when $\\boldsymbol{b}=\\boldsymbol{0}$. \n",
    "\n",
    "So we can make use of the reduced row echelon form of $A$ we created above (call it  $A_{\\text{RREF}}$). \n",
    "\n",
    "The solutions to $A\\boldsymbol{b}=\\boldsymbol{0}$ will be the solutions to $A_{\\text{RREF}}\\boldsymbol{b}=\\boldsymbol{0}$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 6 \\\\\n",
    "0 & 1 & 2 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "\\iff \n",
    "\\left\\{\n",
    "\\begin{align}\n",
    "1x + 0y + 6z &= 0\\\\\n",
    "0x + 1y + 2z &= 0\\\\\n",
    "0x + 0y + 0z &= 0\n",
    "\\end{align}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "from which we can conclude that we actually have here two equations for three unknowns - we expect infinitely many solutions.\n",
    "\n",
    "Let's set $x = \\alpha$, with $\\alpha\\in\\mathbb{R}$ an arbitrary scalar. \n",
    "\n",
    "Then the first equation tells us that $z = -\\alpha / 6$, and then the second that $y = \\alpha / 3$. This will be a solution for any $\\alpha$, and so in this example the dimension of the null space is 1 (and the dimension of the null space plus the rank is $n=3$ as expected).\n",
    "\n",
    "For example, with one choice of the arbitrary $\\alpha$ we get one vector from the null-space:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "6 \\\\\n",
    "2 \\\\\n",
    "-1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Can we do this in `numpy`, and check our solution against?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[-2, -4, -20], [2 , 6 , 24], [2, 10, 32]])\n",
    "null_vecs = sl.null_space(A)\n",
    "print(null_vecs)\n",
    "\n",
    "# check that A@ these vectors yields the zero vector\n",
    "print('A@null_vecs = 0?',np.allclose( A@null_vecs, np.array( [0,0,0] )))\n",
    "\n",
    "# is this the same as we obtained above - let's normalise it and multiply it by the length of the vector above\n",
    "print((sl.null_space(A) / sl.norm(sl.null_space(A))) * sl.norm(np.array([6,2,-1])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can always multiple the vectors in the null space by -1 (or indeed any constant) and it is still a member of the null space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An overview of over-, under-, equi- and mixed-determined problems\n",
    "\n",
    "Consider the linear inversion problem represented by the equation \n",
    "\n",
    "$$ \\quad A\\, \\boldsymbol{x} = \\boldsymbol{b} $$\n",
    "\n",
    "where $A$ is an $m \\times n$ matrix, we can formally identify several possible circumstances:  \n",
    "\n",
    "\n",
    "\n",
    "## The equi-determined case\n",
    "\n",
    "<img src=\"./latex/matrix-square.png\" width=400x>\n",
    "\n",
    "There are exactly as many data points (in $\\boldsymbol{b}$) as model parameters (in $\\boldsymbol{x}$). \n",
    "\n",
    "The  matrix  $A$  will  be  square  so  that  $m  =  n$. \n",
    "\n",
    "There will be as many equations as unknowns.  \n",
    "\n",
    "<br>\n",
    "\n",
    "If these equations are both *independent*\n",
    "\n",
    "[i.e. the \"lines\" we saw previously in $2\\times 2$ cases corresponding to each equation/constraint are not parallel] \n",
    "\n",
    "are and *consistent*\n",
    "\n",
    "[which means that the lines cross at least once; inconsistency is where we can derive a contradiction from the equations, e.g. using row operations to arrive at the equation $0=1$, i.e. the rank of the augmented matrix is greater than the rank of the coefficient matrix - see next cell] \n",
    "\n",
    "then the problem is called ***equi-determined***.  \n",
    "\n",
    "There will be one and only one solution that will fit the data (the RHS vector) exactly.  \n",
    "\n",
    "<br><br>\n",
    "\n",
    "For further discussion of this see <https://en.wikipedia.org/wiki/System_of_linear_equations#Properties>\n",
    "\n",
    "also ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside - the Rouché–Capelli theorem\n",
    "\n",
    "The [Rouché–Capelli theorem](https://en.wikipedia.org/wiki/Rouch%C3%A9%E2%80%93Capelli_theorem) tells us that  \n",
    "\n",
    "\"any system of equations ... is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions.\"\n",
    "\n",
    "Above quoted from <https://en.wikipedia.org/wiki/Overdetermined_system#Non-homogeneous_case>\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "The following is a simple example of an inconsistent system:\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "x + y + z = 0 \\\\[5pt]\n",
    "x + y + z = 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "if you form the augmented matrix here and perform row operations, the rank of the augmented matrix will clearly be larger than the rank of $A$ alone. Just drop the $z$ from these equations and we have an inconsistent square system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The over-determined case\n",
    "\n",
    "<img src=\"./latex/matrix-tall.png\" width=300x>\n",
    "\n",
    "\n",
    "In this case there are more independent (i.e. ignoring repeated equations) data points than there are model parameters.\n",
    "\n",
    "The matrix $A$ will be a \"tall\" matrix with more rows than columns so that $m > n$.  \n",
    "\n",
    "Equivalently, there are more equations (or constraints) than unknowns. \n",
    "\n",
    "If $A$ is full rank (recall that this means that $\\text{rank}(A) = \\min(m,n)= n$), then the inversion problem is called ***purely over-determined***.  \n",
    "\n",
    "\n",
    "Typically there will be no exact solution to an over-determined problem.   \n",
    "\n",
    "In the purely over-determined case we can use our formula above for the least squares solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The under-determined case\n",
    "\n",
    "\n",
    "<img src=\"./latex/matrix-fat.png\" width=400x>\n",
    "\n",
    "\n",
    "In this case there  are  fewer data points than model parameters.  \n",
    "\n",
    "The matrix $A$ will be a \"fat\" matrix with fewer rows than columns so that $m < n$.  \n",
    "\n",
    "Equivalently, there are less equations (or constraints) than unknowns. \n",
    "\n",
    "If  $A$  is  full  rank (recall that this means that $\\text{rank}(A) = \\min(m,n) = m$), then the problem is called ***purely under-determined***. \n",
    "\n",
    "There will be infinitely many solutions that are able to fit the data exactly.\n",
    "\n",
    "If an under-determined case is **not** purely under-determined (i.e. $\\text{rank}(A) < m$), then this means that we can't find solutions that exactly fit the data.\n",
    "\n",
    "The following is an example as it is indeed under-determined but has no exact solution(s) as the equations are inconsistent:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "2 & 4 & 6\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "m_1\\\\\n",
    "m_2\\\\\n",
    "m_3\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The mixed-determined case\n",
    "\n",
    "\n",
    "Many real problems will actually be partly over-determined and partly under-determined.  \n",
    "\n",
    "These problems  are  called  ***mixed-determined***. \n",
    "\n",
    "Problems  in  which  some  but  not  all  of  the parameters  are  equi-determined  are  also  mixed  determined. \n",
    "\n",
    "In  mixed-determined problems,  some of the model parameters (i.e. some components of the unknown $\\boldsymbol{x}$)  will  typically  be  described  by  more independent equations than there are parameters and there will be no model that can satisfy this set of equations, \n",
    "\n",
    "while some other parameters will be described by fewer independent equations than there are parameters and there may be be infinitely many models that can satisfy this second set of equations.  \n",
    "\n",
    "A mixed-determined problem can occur with any shaped matrix $A$.  \n",
    "\n",
    "There will typically be no exact solution to this problem.  In addition, there will be infinitely many solutions that can fit the data equally accurately. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples\n",
    "\n",
    "### Example 1  <a class=\"tocSkip\">\n",
    "\n",
    "Consider the following problem:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 2 & 2 \\\\\n",
    "0 & 3 & 3\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_3\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "2\\\\\n",
    "2\\\\\n",
    "3\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "For this problem, $x_1$ is over-determined since there is no possible value of $x_1$ that can exactly fit both of the first two equations in this system - the first two equations/constraints are inconsistent. \n",
    "\n",
    "The problem is also under-determined because the last two equations are not independent. In the particular case as written here the final two equations actually boil down to one equation for two unknowns. Thus, there are infinitely many solutions for $x_2$ and $x_3$ that can fit the last two equations exactly.  \n",
    "\n",
    "This problem is therefore clearly ***mixed-determined***.  \n",
    "\n",
    "A homework exercise asks you to come up with a sensible/reasonable \"solution\" to this problem.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Most, large, real-world, inversion problems are likely to be mixed-determined.  In practice it can also be very difficult to discover if a large system is mixed determined or not, and so in practical problems we will often proceed by assuming that the problem is mixed-determined, employing an iterative numerical approach.   \n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Example 2  <a class=\"tocSkip\">\n",
    "\n",
    "The following figure illustrates these four problem types for a simple tomography experiment - we want to work out the density of each block from the time it takes a wave to pass through the blocks along different paths. \n",
    "\n",
    "The \"s\" values plotted just indicate that some measurement has been taken, and we want to use this to establish values in the two blocks.\n",
    "\n",
    "\n",
    "<img src=\"./figures/over_under_mixed_determined_example.png\" width=600x>\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "- In the first case we have exactly enough information to solve our inversion problem exactly.\n",
    "\n",
    "\n",
    "- In the second case we can only describe the mean of the parameter values, but without additional information there is an infinite family of possible individual parameter values that agree with our data - we have an under-determined problem.\n",
    "\n",
    "\n",
    "- In the third we are given the parameters in each rectangle and additionally the mean - we have an over-determined problem; we may or may not be able to find a solution that satsifies all observations/equations.\n",
    "\n",
    "\n",
    "- In the fourth we are only told the mean (so are under-determined as far as the individual block values, or their difference, are concerned - so these quantities are under-determined) but we are told that mean value three times (so we are over-determined in terms of the mean - an issue if the three data values are different). We could transform this problem into one for two new parameters - the mean and the difference - where we have three equations for the first parameter and none for the second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equi-determined problems - square linear systems\n",
    "\n",
    "For a linear inversion problem that has the same number of data points as model parameters, the equation \n",
    "\n",
    "$$A\\, \\boldsymbol{x} = \\boldsymbol{b}$$\n",
    "\n",
    "contains a square matrix $G$, and the equation represents a set of linear simultaneous equations that has the same number of equations as unknowns.   \n",
    "\n",
    "<img src=\"./latex/matrix-square.png\" width=300x>\n",
    "\n",
    "Recall: If $A$ is not singular, then equivalently these linear equations will all be independent, and this system will have only one solution; that solution will provide an exact fit to the data.  This solution is given by \n",
    "\n",
    "$$\\boldsymbol{x} = A^{-1}\\,\\boldsymbol{b}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**CAUTION:** The statement that the solution is exact is a mathematical not a physical statement. That is, if the data contains uncertainties, which they always will, then those uncertainties will influence the solution.  A model that provides an exact fit to an observed noisy dataset may not provide a good estimate of the true model even though the estimated model exactly predicts these observations. \n",
    "\n",
    "That is, the issue of small perturbations in the data potentially leading to large errors in the model - this is a concept termed ***ill-conditioning*** which we saw in ACSE3 which occurs when the [***condition number***](https://en.wikipedia.org/wiki/Condition_number) of the matrix is large.\n",
    "\n",
    "<br>\n",
    "\n",
    "If the matrix $A$ is singular, then the inverse of $A$ does not exist, and we cannot use the above equation to find a solution.  \n",
    "\n",
    "In these circumstances, there are two possibilities: either the equations are not independent in which case there will be  infinitely many solutions that exactly fit the data, or the equations are incompatible in which case there will be no solution at all that exactly fits the data. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "These are the cases for a $2\\times 2$ singular case where the equations could be interpreted as two lines that are on top of one another, or as two lines that never cross.\n",
    "\n",
    "In the former case we are actually under-determined (as we don't really have two independent pieces of information) and can find the so-called **minimum norm solution** out of the infinitely many possible solutions - defined shortly.\n",
    "\n",
    "In the latter case there are no exact solutions, but there are infinitely many solutions that are equally valid in terms of a **least squares approach**, we could select from these the one that is **also the minimum norm solution** - this perhaps gives a hint for how we will generally deal with mixed-determined problems! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over-determined problems\n",
    "\n",
    "In over-determined problems, there are more independent equations than unknowns.  That is, in the system  \n",
    "\n",
    "$$A\\, \\boldsymbol{x} = \\boldsymbol{b}$$\n",
    "\n",
    "$m>n$  where $A$ is an $m\\times n$ matrix.\n",
    "\n",
    "<img src=\"./latex/matrix-tall.png\" width=300x>\n",
    "\n",
    "<br>\n",
    "\n",
    "The inverse of $A$ is not defined, and in general there is no solution $\\boldsymbol{x}$ that will exactly satisfy this relation \n",
    "\n",
    "BUT we can find an $\\boldsymbol{x}$ if the data $\\boldsymbol{b}$ lies in the range of $A$ but this won't generally be the case. \n",
    "\n",
    "<br> \n",
    "\n",
    "A useful solution can still be found in the general case, and we've already seen how:\n",
    "\n",
    "Instead of solving our original problem, we instead solve the related equation\n",
    "\n",
    "$$A^T \\,A\\, \\boldsymbol{x} = A^T \\,\\boldsymbol{b}$$\n",
    "\n",
    "This relation if called the **normal equation**.\n",
    "\n",
    "<br>\n",
    "\n",
    "The $n\\times n$ matrix $A^T \\,A$ is now square and symmetric, and  provided that it is not singular, the solution to the normal equation will be \n",
    " \n",
    "$$\\boldsymbol{x} = (A^T \\,A)^{-1}A^T \\,\\boldsymbol{b}$$\n",
    "\n",
    "This approach generates the **least squares solution** to the problem.\n",
    "\n",
    "We've seen examples of this already in both the linear system case as well as in terms of linear regression (fitting a polynomial to data).\n",
    "\n",
    "Let's now establish where this expression for the least squares solution comes from:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "## The least squares solution - derivation [*]\n",
    "\n",
    "We are in the over-determined situation. Since (in general) there is no vector of model parameters that exactly fits the data, we will proceed by trying to find a model that minimises the difference between the model predictions and the observed data. For a good model, we would like this difference to be less than the errors in the data.   \n",
    "\n",
    "There are many ways to compare a predicted and an observed dataset. The one that we will use here is to try to minimise the sum of the squares of the differences between the predicted and observed data. That is, we will minimise the square of the $L^2$-norm of the data residuals.  \n",
    "\n",
    "[A homework exercise walks you through this example in the case of linear regression fitting data, and how the result responds to the presence of an outlier in the data in this and other choices of norm].\n",
    "\n",
    "Thus, if the predicted data are $\\boldsymbol{p}:=A\\boldsymbol{x}$ and the observed data are $\\boldsymbol{b}$, then we will try to minimise the function \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f &= \\frac{1}{2}(\\boldsymbol{p} - \\boldsymbol{b})^T(\\boldsymbol{p} - \\boldsymbol{b})\\\\\n",
    "&=\\frac{1}{2}(A\\boldsymbol{x} - \\boldsymbol{b})^T(A\\boldsymbol{x} - \\boldsymbol{b})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "[NB. The factor of a half has just been added here so that later results simplify.]\n",
    "\n",
    "<br>\n",
    "\n",
    "equivalently using norms\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f&=\\frac{1}{2}\\|\\boldsymbol{p} - \\boldsymbol{b}\\|_2^2\\\\\n",
    "&=\\frac{1}{2}\\|A\\boldsymbol{x} - \\boldsymbol{b}\\|_2^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "or in terms of scalar components:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f&=\\frac{1}{2}\\sum_{j=1}^m (p_j - b_j)^2\\\\\n",
    "&=\\frac{1}{2}\\sum_{j=1}^m \\left( \\sum_{k=1}^n A_{jk}x_k - b_j \\right)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Here, $f$ is just a non-negative number that will vary as the model varies.  The function $f$ is usually called the ***objective function***.  It is also often called the ***misfit function*** for obvious reasons. In an optimisation based approach to solve inversion problems, we often use iterative approaches to minimise the objective/misfit function.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "As we've seen above, to minimise $f$ we need to differentiate $f$ with respect to each of the model parameters $m_k$, set these differentials to zero, and solve the resulting set of equations for these model parameters (which is done in a homework exercise).  \n",
    "\n",
    "Now we have $n$ model parameters and $n$ equations (one for each differential), so that we have the same number of equations as unknowns.  Now we can get a unique solution - at least we can provided that these $n$ equations are independent and consistent.  \n",
    "\n",
    "The solution that this will generate will not exactly explain the data - there is no solution that can do that for arbitrary data. It is instead the least-squares solution, a solution that gives the best fit to the observed data in a least-squares sense. It minimises the sum of the squares of the residual data  (the  difference  between  observed  and  predicted  data). It  minimises  the  Euclidean distance between the observed and predicted data vectors.  \n",
    "\n",
    "Some theory does exist to establish that the least squares solution is optimal assuming certain assumptions on the errors in the data apply - see <https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem>.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "To progress we can make use of a vector calculus result:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\boldsymbol{x}} \\left(\\boldsymbol{a}^T\\boldsymbol{b}\\right) \n",
    "=\\left(\\frac{\\partial \\boldsymbol{a}}{\\partial \\boldsymbol{x}}\\right)^T\\boldsymbol{b} +\n",
    "\\left(\\frac{\\partial \\boldsymbol{b}}{\\partial \\boldsymbol{x}}\\right)^T\\boldsymbol{a}$$\n",
    "\n",
    "You're asked to think about this in a homework example.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "Consider now a function of the form \n",
    "\n",
    "$$f = \\frac{1}{2}\\|\\boldsymbol{r}\\|_2^2 = \n",
    "\\frac{1}{2}\\boldsymbol{r}^T \\boldsymbol{r}\n",
    "$$\n",
    "\n",
    "where both $\\boldsymbol{r}$ and $f$ are functions of the model parameters $\\boldsymbol{x}$, and $\\boldsymbol{r}$ is some measure of the mismatch between the data that is observed and the data that would be generated by using the model parameters $\\boldsymbol{x}$.  Now we will see why the factor of a half is added to simplify the final result.    \n",
    "\n",
    "The classic least-squares problem then involves discovering the model parameters that minimises the value of the objective function $f$, and this typically involves differentiating $f$ with respect to $\\boldsymbol{x}$.  \n",
    "\n",
    "Using the result above for the differential of an inner product, we obtain  \n",
    "\n",
    "$$\\frac{\\partial f}{\\partial \\boldsymbol{x}}\n",
    "= \\frac{\\partial }{\\partial \\boldsymbol{x}}\\left(\\frac{1}{2}\\boldsymbol{r}^T \\boldsymbol{r}\\right)\n",
    "= \\frac{1}{2}\\left(\\left(\\frac{\\partial \\boldsymbol{r}}{\\partial \\boldsymbol{x}}\\right)^T\\boldsymbol{r} +\n",
    "\\left(\\frac{\\partial \\boldsymbol{r}}{\\partial \\boldsymbol{x}}\\right)^T\\boldsymbol{r}\\right)\n",
    "= \\left(\\frac{\\partial \\boldsymbol{r}}{\\partial \\boldsymbol{x}}\\right)^T\\boldsymbol{r}\n",
    "$$\n",
    "\n",
    "Let's now apply this to our situation:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "Recall we had\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f &= \\frac{1}{2}(\\boldsymbol{p} - \\boldsymbol{b})^T(\\boldsymbol{p} - \\boldsymbol{b})\\\\\n",
    "&=\\frac{1}{2}(A\\boldsymbol{x} - \\boldsymbol{b})^T(A\\boldsymbol{x} - \\boldsymbol{b})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Lets differentiate with respect to $\\boldsymbol{x}$ and use our vector calculus result from just above\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial \\boldsymbol{x}} = \\frac{\\partial (A\\boldsymbol{x} - \\boldsymbol{b})^T}{\\partial \\boldsymbol{x}} (A\\boldsymbol{x} - \\boldsymbol{b})$$\n",
    "\n",
    "which, since $\\boldsymbol{b}$ and $A$ do not depend on $\\boldsymbol{x}$, gives \n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial \\boldsymbol{x}} = A^T(A\\boldsymbol{x} - \\boldsymbol{b})$$\n",
    "\n",
    "For $f$ to be a minimum, this must be equal to zero, so we are looking for the $\\boldsymbol{x}$ where\n",
    "\n",
    "$$A^T A  \\boldsymbol{x} - A^T \\boldsymbol{b}=0 \\qquad \\iff \\qquad\n",
    "A^T A  \\boldsymbol{x} = A^T \\boldsymbol{b}\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$ \\boldsymbol{x} = (A^T A)^{-1} A^T  \\boldsymbol{b} $$\n",
    "\n",
    "provided that $(A^T A)^{-1}$ exists.\n",
    "\n",
    "Note that this is the normal equation we defined above!\n",
    "\n",
    "Here $A$ is a rectangular $m\\times n$ matrix where for our \"tall\" over-determined case $m>n$. However,  $A^TA$ is a square matrix of size $n\\times n$. Its inverse will exist provided that $A$ is of full column rank, that is provided that the problem is purely over-determined.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "### Rank of matrix products [*] <a class=\"tocSkip\">\n",
    "\n",
    "Note that a matrix product always has a rank that is less than or equal to the smallest rank of any of the constituent  matrices.    For  example,  the  outer  product  $\\boldsymbol{a}\\boldsymbol{b}^T$  of  two column  vectors  must  be  rank  1 because vectors are only rank 1. \n",
    "\n",
    "<br>\n",
    "\n",
    "You could check this with an example - see homework.\n",
    "\n",
    "<br>\n",
    "\n",
    "We have several important results:\n",
    "\n",
    "\n",
    "- A square $(m = n)$  matrix is full rank if and only if it is non-singular.  \n",
    "\n",
    "\n",
    "- For a tall $(m > n)$ matrix $A$ that is full rank, the matrix formed by $A^TA$ (which has dimension $n \\times n$) will be non-singular.  \n",
    "\n",
    "\n",
    "- For a fat $(m < n)$ matrix $A$ that is full rank, the matrix formed by $AA^T$ (which has dimension $m \\times m$) will be non-singular. \n",
    " \n",
    " \n",
    "- If $A$ is not full rank, then whatever the shape of $A$, both these matrices will be singular.  \n",
    "\n",
    "\n",
    "The first three cases correspond respectively to equi-, purely over-, and purely under- determined inversion problems. The mixed-determined case will fall into category 4.\n",
    "    \n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The left inverse (or the least-squares inverse) <a class=\"tocSkip\">\n",
    "\n",
    "Note that the expression $(A^T A)^{-1} A^T$ has similarities to an inverse of the rectangular matrix $A$ since\n",
    "\n",
    "$$\\left( (A^T A)^{-1} A^T \\right)A = I$$\n",
    "\n",
    "However it is not a true inverse since\n",
    "\n",
    "$$A\\left( (A^T A)^{-1} A^T \\right) \\ne I$$\n",
    "\n",
    "It is often called the [**left inverse**](https://en.wikipedia.org/wiki/Inverse_element#Matrices) (or the least-squares inverse) of $A$.\n",
    "\n",
    "In many problems $A$ may not be full rank, so that $A^TA$ is singular. Such problems are normally mixed-determined. We can solve such problems using the method of *damped least squares* (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under-determined problems\n",
    "\n",
    "In under-determined problems,  there are fewer  equations than unknowns, and so  $m$  is  less than $n$.  \n",
    "\n",
    "<img src=\"./latex/matrix-fat.png\" width=400x>\n",
    "\n",
    "Now the equations do not uniquely define a solution. \n",
    "\n",
    "However, in this case too there is still a useful solution to be found.  \n",
    "\n",
    "When $m<n$,  the  matrix  $A^TA$ will  be  singular (cf. the rank of matrix products),  so  we  cannot  proceed  as  for  the  over-determined case.  \n",
    "\n",
    "However, the similar $m \\times m$ matrix $AA^T$ will not be singular if the equations are independent and consistent - if $A$ is full rank.  \n",
    "\n",
    "We can therefore construct the solution\n",
    "\n",
    "$$\\boldsymbol{x} = A^T(AA^T)^{-1}\\boldsymbol{b}$$\n",
    "\n",
    "where this time the matrix $A^T(AA^T)^{-1}$ can be regarded as a type of inverse to $G$ since\n",
    "\n",
    "$$A\\left( A^T (AA^T)^{-1} \\right) = I$$\n",
    "\n",
    "\n",
    "- This inverse is called the [***right  inverse***](https://en.wikipedia.org/wiki/Inverse_element#Matrices) of $A$, or the ***minimum-norm  inverse***.    \n",
    "\n",
    "\n",
    "- The minimum-norm inverse generates a perfect fit to the data.  \n",
    "\n",
    "\n",
    "- This however is not the only exact solution.  \n",
    "\n",
    "\n",
    "- This problem has a null space since we have fewer equations than unknowns, and an  infinite  number  of  solutions  will  satisfy the data equally well.    \n",
    "\n",
    "\n",
    "- The  minimum-norm solution is the solution that both fits the data exactly, and having satisfied that constraint then minimises the $L^2$   norm of the model. That  is, subject  to  first  fitting the data, it then also minimises $\\boldsymbol{x}^T\\boldsymbol{x}$.\n",
    "\n",
    "<br>\n",
    "\n",
    "The latter condition makes the model parameter vector as \"short\" as possible (thinking of $\\boldsymbol{x}$ as a vector) given that it must also match the data.  This  is  a  minimum  model parameter vector that  has  nothing  within  it  that  can  be  left  out  without degrading the fit to the data.  However, we can choose to add to it any linear combination from the null space and it will still explain the data exactly. \n",
    "\n",
    "In many circumstances, it is appropriate to parametrise the model so that the parameters that we are attempting to obtain are not defined in an absolute sense, but are defined as changes to some *a priori* model $\\boldsymbol{x}_0$.  \n",
    "\n",
    "The *a priori* model is our best guess of what the answer should be in the absence of the data.  If we parametrise in this way, then a solution using the approach above will ensure that we find the model that best fits the data and that is also as close to the *a priori* model as possible.  \n",
    "\n",
    "In this case, the problem is often set up to solve for $\\delta \\boldsymbol{x}:=\\boldsymbol{x} - \\boldsymbol{x}_0$, and takes the form\n",
    "\n",
    "$$\\boldsymbol{x} - \\boldsymbol{x}_0  = A^T(AA^T)^{-1}(\\boldsymbol{b} - A \\boldsymbol{x}_0)$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\delta \\boldsymbol{x} = A^T(AA^T)^{-1}\\delta \\boldsymbol{b}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\delta \\boldsymbol{b}=\\boldsymbol{b} - A \\boldsymbol{x}_0$$\n",
    "\n",
    "is the difference between the observed data and that predicted using the starting model $\\boldsymbol{x}_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "## Derivation of the minimum norm solution [*]\n",
    "\n",
    "Mathematically we can state that we want to find the solution $\\boldsymbol{x}$ that satisfies our problem $A\\boldsymbol{x} = \\boldsymbol{b}$, subject to that solution being as small as possible, i.e. such that  $\\boldsymbol{x}^T\\boldsymbol{x}$ is minimsed.\n",
    "\n",
    "As a constrained optimisation problem we would write\n",
    "\n",
    "\\begin{align}\n",
    "\\textrm{minimise} \\quad & \\boldsymbol{x}^T\\boldsymbol{x}\\\\\n",
    "\\textrm{subject to} \\quad & A\\boldsymbol{x} = \\boldsymbol{b}\n",
    "\\end{align}\n",
    "\n",
    "A standard approach to solve problems such as this is via [*Lagrange multipliers*](https://en.wikipedia.org/wiki/Lagrange_multiplier):\n",
    "\n",
    "define the so-called *Lagrangian function*\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{x}, \\boldsymbol{\\lambda}) :=  \\boldsymbol{x}^T\\boldsymbol{x} - \\boldsymbol{\\lambda}^T (A\\boldsymbol{x} - \\boldsymbol{b})\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\lambda}$ is the Lagrange multiplier that is introduced to enforce the constraint - here the constraint is vector valued and so $\\boldsymbol{\\lambda}$ is a vector of Lagrange multipliers. \n",
    "\n",
    "We now look for the stationary points of $\\mathcal{L}$, i.e. those for which the derivative w.r.t. $\\boldsymbol{x}$ (here a gradient vector) is zero, and where the derivative w.r.t. $\\boldsymbol{\\lambda}$ is zero (again a gradient vector; the latter being satsified just means that each of the components of our vector constraint are satisfied of course, and so the stationary points of $\\mathcal{L}$ are always points in $(\\boldsymbol{x}, \\boldsymbol{\\lambda})$ space where our equation is exactly satsified.\n",
    "\n",
    "In this case this means that we want\n",
    "\n",
    "$$\\boldsymbol{0}=\\nabla_{\\boldsymbol{x}}\\mathcal{L} = 2\\boldsymbol{x} - A^T\\boldsymbol{\\lambda}$$\n",
    "\n",
    "\n",
    "[demonstrating this result is a homework exercise]\n",
    "\n",
    "and\n",
    "\n",
    "$$\\boldsymbol{0}=\\nabla_{\\boldsymbol{\\lambda}}\\mathcal{L} = A\\boldsymbol{x} - \\boldsymbol{b}$$\n",
    "\n",
    "Substituting the first into the second we have\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{0} &= A\\boldsymbol{x} - \\boldsymbol{b} = A\\left(\\frac{1}{2}A^T\\boldsymbol{\\lambda}\\right) - \\boldsymbol{b}\\\\\n",
    "\\Rightarrow\n",
    "\\boldsymbol{\\lambda} &= 2 (A A^T)^{-1}\\boldsymbol{b}\n",
    "\\end{align}\n",
    "\n",
    "and substituting back into the first:\n",
    "\n",
    "$$\\boldsymbol{x} = \\frac{1}{2} A^T\\boldsymbol{\\lambda} = A^T (A A^T)^{-1}\\boldsymbol{b}. $$\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed-determined problems\n",
    "\n",
    "Recall our simple mixed determined problem from above\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 2 & 2 \\\\\n",
    "0 & 3 & 3\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_3\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "2\\\\\n",
    "2\\\\\n",
    "3\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "None of our solution methods are applicable to this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just confirm that A.T@A and A@A.T are singluar \n",
    "A = np.array([\n",
    "    [1,0,0],\n",
    "    [1,0,0],\n",
    "    [0,2,2],\n",
    "    [0,3,3]])\n",
    "\n",
    "print(sl.det(A.T@A))\n",
    "print(sl.det(A@A.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible solution methods\n",
    "\n",
    "In  this  problem,  $A$  is  not  square  so  that  $A^{-1}$ does not exist,  and  both  $A^TA$ and $AA^T$ are singular matrices, so that none of the methods that we have used so far will work.  \n",
    "\n",
    "So what can we do?  There are two principal options: \n",
    "\n",
    "\n",
    "1. we can use the ***generalised inverse*** $A^+$, also known as the pseudo-inverse or the [Moore-Penrose inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse), or\n",
    "\n",
    "\n",
    "2. we can use some form of ***regularisation*** to the model of which ***damped least-squares*** is the most straightforward.   \n",
    "\n",
    "\n",
    "The generalised inverse is  preferable in  small problems, especially  when we would like to analyse the quality of the results carefully, while regularised least-squares and related methods are preferable for large problems when the generalised inverse is prohibitively expensive, or when linearised inversion is being used in order to solve a non-linear problem by iteration.  \n",
    "\n",
    "<br>\n",
    "\n",
    "For the former of these we need to introduce the Singular Value Decomposition (SVD), but first let's review the eigenvalue decomposition for square matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "# Matrix diagonalisation - eigenvalue decomposition reminder [*]\n",
    "\n",
    "The SVD can be seen as a generalisation of the eigenvalue decomposition, which has similarities to the result obtained for symmetric matrices, but that is applicable to all matrices of any shape.\n",
    "\n",
    "Let's quickly remind ourselves of the eigenvalue decomposition of a square matrix.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "## Some useful theoretical results about eigenvalues/vectors\n",
    "\n",
    "Recall that\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. The eigenvalues of symmetric real matrices are *always real*.\n",
    "\n",
    "\n",
    "\n",
    "2. Eigenvectors corresponding to distinct eigenvalues are always *linearly independent*. (For repeated eigenvalues (i.e. algebraic multiplicity greater than 1) we **may be able** to find the corresponding number of linearly independent eigenvectors).\n",
    "\n",
    "\n",
    "\n",
    "3. The eigenvectors of a symmetric real matrix, corresponding to *distinct* eigenvalues, are mutually *orthogonal*.\n",
    "\n",
    "\n",
    "\n",
    "4. A consequence of this is that, if a symmetric real matrix has all distinct eigenvalues, then \n",
    "    - the matrix of normalised (i.e. of unit length) eigenvectors is orthogonal: $P^TP = I$. (If we don't bother to normalise them first then $P^TP$ would instead be diagonal matrix).\n",
    "    - the normalised vectors form an *orthonormal basis* for $n$-dimensional Euclidean space.\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "## Matrix diagonalisation\n",
    "\n",
    "A further consequence/related concept is that a square $n \\times n$ matrix $A$ with $n$ **distinct** eigenvalues is diagonalisable.\n",
    "\n",
    "This means that we can find a matrix $P$ such that we decompose or factorize the matrix in the following way\n",
    "\n",
    "$$A = P\\Lambda P^{-1}$$\n",
    "\n",
    "where $\\Lambda$ is a diagonal matrix. \n",
    "\n",
    "Said another way, there exists a transformation matrix $P$ such that\n",
    "\n",
    "$$P^{-1}AP\\quad\\text{is a diagonal matrix}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that the converse is not true - a matrix **may be** diagonalisable even if it does not have distinct eigenvalues, i.e. has repeated eigenvalues.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "### The case of symmetric real matrices <a class=\"tocSkip\">\n",
    "\n",
    "In the special case of a symmetric real matrix, it can ***always*** *be diagonalised* using an [orthogonal matrix](https://en.wikipedia.org/wiki/Orthogonal_matrix):\n",
    "\n",
    "$$A = Q\\Lambda Q^T \\quad \\iff \\quad \\Lambda = Q^T A Q$$\n",
    "\n",
    "A consequence is that any $n\\times n$ real symmetric matrix must possess $n$ mutually orthogonal eigenvectors, even if it has repeated eigenvalues (so we actually have a stronger result than 4 above).\n",
    "\n",
    "[Aside: The slight contradiction in this phrasing you may spot here is due to the fact that the eigenvectors corresponding to repeated eigenvalues, while chosen to be l.i. in order to span the corresponding eigenspace, won't be automatically orthogonal. However, we can construct or choose orthogonal eigenvectors - the [Gram-Schmidt process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process) is a method that can be used to achive this].\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "### When is a matrix diagonalisable <a class=\"tocSkip\">\n",
    "\n",
    "An $n \\times n$ matrix $A$ is diagonalisable if and only if for all eigenvalues the algebraic and geometric multiplicities are equal, equivalently the sum of the geometric multiplicities is $n$ - we'll see why in the next cell. \n",
    "\n",
    "This just means we need to be able to find $n$ linearly independent eigenvectors (even if some of the eigenvalues are repeated).\n",
    "\n",
    "For more details see <https://en.wikipedia.org/wiki/Diagonalizable_matrix>.\n",
    "\n",
    "What if we can't find $n$ l.i. eigenvectors? Then the [Jordan normal form](https://en.wikipedia.org/wiki/Jordan_normal_form)  is the best we can do, which is still useful for certain tasks.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "### The details <a class=\"tocSkip\">\n",
    "\n",
    "Why do we need linearly independent eigenvectors?\n",
    "\n",
    "Consider an $n\\times n$ matrix $A$ with eigenvalues $\\boldsymbol{v}_i$ and corresponding eigenvalues $\\lambda_i$, $i=1,\\ldots, n$.\n",
    "\n",
    "Define $P$ to be the matrix of eigenvectors making up its columns:\n",
    "\n",
    "$$P = \\left[ \\; \\boldsymbol{v}_1 \\;\\;  \\boldsymbol{v}_2  \\; \\; \\cdots \\; \\;  \\boldsymbol{v}_n \\; \\right]$$\n",
    "\n",
    "and $\\Lambda$ the diagonal matrix of eigenvectors:\n",
    "\n",
    "$$\\Lambda = \\begin{pmatrix}\n",
    "\\lambda_1 & 0 &  \\cdots & 0 \\\\\n",
    "0 & \\lambda_2 &  \\ddots & 0\\\\\n",
    "\\vdots & &  \\ddots  & \\vdots \\\\\n",
    "0 & \\cdots & 0 & \\lambda_n\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Then based on what we've seen about matrix-vector multiplication several times already, i.e. it acts on the columns independently, we have\n",
    "\n",
    "\\begin{align*}\n",
    "AP & = A \\left[ \\; \\boldsymbol{v}_1 \\;\\;  \\boldsymbol{v}_2  \\; \\; \\cdots \\; \\;  \\boldsymbol{v}_n \\; \\right] \\\\[5pt]\n",
    "& = \\left[ \\;  \\lambda_1\\boldsymbol{v}_1 \\;\\;   \\lambda_2\\boldsymbol{v}_2  \\; \\; \\cdots \\; \\;  \\lambda_n\\boldsymbol{v}_n \\; \\right] \\\\[15pt]\n",
    "& = \\left[ \\; \\boldsymbol{v}_1 \\;\\;  \\boldsymbol{v}_2  \\; \\; \\cdots \\; \\;  \\boldsymbol{v}_n \\; \\right]\n",
    "\\begin{pmatrix}\n",
    "\\lambda_1 & 0 &  \\cdots & 0 \\\\\n",
    "0 & \\lambda_2 &  \\ddots & 0\\\\\n",
    "\\vdots & &  \\ddots  & \\vdots \\\\\n",
    "0 & \\cdots & 0 & \\lambda_n\n",
    "\\end{pmatrix}\\\\[15pt]\n",
    "& = P\n",
    "\\begin{pmatrix}\n",
    "\\lambda_1 & 0 &  \\cdots & 0 \\\\\n",
    "0 & \\lambda_2 &  \\ddots & 0\\\\\n",
    "\\vdots & &  \\ddots  & \\vdots \\\\\n",
    "0 & \\cdots & 0 & \\lambda_n\n",
    "\\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "<br>\n",
    "\n",
    "This is good, but to finish things and to obtain the diagonalisation we are reliant on $P$ being invertible. \n",
    "\n",
    "$P$ is the matrix of eigenvectors. \n",
    "\n",
    "$P$ is therefore invertible (and we thus have a diagonalisation of the matrix) if and only if the eigenvectors are linearly independent. This is one of the equivalent properties of a matrix being invertible that we saw earlier.\n",
    "\n",
    "In which case we post-multiply both sides by $P^{-1}$:\n",
    "\n",
    "\\begin{align*}\n",
    "AP = P \\Lambda \\implies A = P \\Lambda P^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "### An example <a class=\"tocSkip\">\n",
    "\n",
    "Let's consider an example, use the eigenvalues and eigenvectors to construct $P$ and $\\Lambda$ and check that their combination is indeed equal to $A$:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[3., 1.], [1.,3.]])\n",
    "\n",
    "# we worked out the eigenvalues and eigenvectors in ACSE2\n",
    "lam1 = 4\n",
    "# use the normalised e-vector\n",
    "v1 = np.array([1./np.sqrt(2), 1./np.sqrt(2)])\n",
    "lam2 = 2\n",
    "# use the fact we know the e'vecs are orthogonal to construct the second\n",
    "v2 = np.array([-v1[1],v1[0]])\n",
    "\n",
    "# force v1 and v2 to be column vecs\n",
    "v1.shape = (2,1)\n",
    "v2.shape = (2,1)\n",
    "\n",
    "# and stack them into the matrix P\n",
    "P = np.hstack((v1,v2))\n",
    "# and form the diagonal matrix with e'vals on the main diagonal\n",
    "Lambda = np.diag(np.array([lam1,lam2]))\n",
    "\n",
    "# we can check if our result is (approximately - read the docs) correct with\n",
    "print('A = P \\Lambda P^{-1}:  ', np.allclose(A , P@Lambda@(sl.inv(P))))\n",
    "print('A = P \\Lambda P^T:  ', np.allclose(A , P@Lambda@((P.T))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "Now let's check how we can form the diagonalisation using the outputs from `sl.eig`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam, vecs = sl.eig(A)\n",
    "print('The P we constructed is the same matrix you get from sl.eig:', np.allclose(P , vecs))\n",
    "print('The diagonal Lambda matrix we constructed is the same matrix you get from sl.eig:', np.allclose(Lambda , lam))\n",
    "print(' Our Lambda matrix = \\n ',Lambda)\n",
    "print('sl.eig returns for eigenvalues: ',lam)\n",
    "# ah we forgot that eig returns complex numbers even when real, so instead let's check\n",
    "print('The diagonal Lambda matrix we constructed is the same matrix you get via np.diag(np.real(lam)):', \n",
    "      np.allclose(Lambda , np.diag(np.real(lam))))\n",
    "# and therefore now\n",
    "print('A =  vecs@np.diag(np.real(lam))@sl.inv(vecs)) :', np.allclose(A, vecs@np.diag(np.real(lam))@sl.inv(vecs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "## A geometrical interpretation\n",
    "\n",
    "Let's consider how all vectors (points) of a fixed unit magnitude transform by considering how a unit circle transforms under multiplication by an arbitrary matrix $A$.\n",
    "\n",
    "Imagine the circle made up of a collection of points, each representing the vector from the origin. If we transform each of these points and maintain their ordering (i.e. maintain the lines joining them which in $\\mathbb{R}^n$ approximate a continuous circle), what shape do we arrive at in $\\mathbb{R}^m$ (where here $n=m=2$)? \n",
    "\n",
    "In 3D we can of course think about a unit sphere, and so on to even higher dimensions - the same ideas hold for any dimension square system, but things are a little different for non-square systems as we shall see later.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# construct points on a circle parametrically\n",
    "theta = np.linspace(0,2*np.pi,100)\n",
    "np.append(theta, 0) # add a zero at the end so we go back to the start in the plot\n",
    "\n",
    "# A 2xn array of points\n",
    "points = np.vstack( (np.cos(theta), np.sin(theta)) )\n",
    "\n",
    "ax1.set_xlabel(\"$x, x'$\", fontsize=14)\n",
    "ax1.set_ylabel(\"$y, y'$\", fontsize=14)\n",
    "ax1.set_title('Linear transformation', fontsize=14)\n",
    "ax1.grid(True)\n",
    "\n",
    "x = np.array([1,0])\n",
    "y = np.array([0,1])\n",
    "\n",
    "# plot the circle\n",
    "ax1.plot(points[0,:], points[1,:], color='blue')\n",
    "\n",
    "# plot some vectors\n",
    "ax1.quiver(x[0], x[1], angles='xy', scale_units='xy', scale=1, color='b', zorder=10)\n",
    "ax1.quiver(y[0], y[1], angles='xy', scale_units='xy', scale=1, color='b', zorder=10)\n",
    "\n",
    "\n",
    "# choose a random transformation matrix\n",
    "A = np.array([[2.2,0.3],[1.7,2.5]])\n",
    "\n",
    "xp = A@x\n",
    "yp = A@y\n",
    "pointsp = A@points\n",
    "\n",
    "# plot the transformed circle\n",
    "ax1.plot(pointsp[0,:], pointsp[1,:], color='red')\n",
    "\n",
    "# plot the transformed vectors\n",
    "ax1.quiver(xp[0], xp[1], angles='xy', scale_units='xy', scale=1, color='r', zorder=10)\n",
    "ax1.quiver(yp[0], yp[1], angles='xy', scale_units='xy', scale=1, color='r', zorder=10)\n",
    "\n",
    "# plot a subsect of vectors and how they transform\n",
    "for i, angle in enumerate(theta[::5]):  # don't use all the angles\n",
    "    #ax1.plot([0,points[0,5*i]],[0,points[1,5*i]],'k')\n",
    "    ax1.plot([points[0,5*i],pointsp[0,5*i]],[points[1,5*i],pointsp[1,5*i]],'k')\n",
    "    ax1.plot([points[0,5*i]],[points[1,5*i]],'b.')\n",
    "    ax1.plot([pointsp[0,5*i]],[pointsp[1,5*i]],'r.')\n",
    "\n",
    "# compute and plot some special directions!\n",
    "lam, vecs = sl.eig(A)\n",
    "\n",
    "# plot the unit eigenvectors\n",
    "ax1.quiver(vecs[0,0], vecs[1,0], angles='xy', scale_units='xy', scale=1, color='limegreen', zorder=20, width=0.015)\n",
    "ax1.quiver(vecs[0,1], vecs[1,1], angles='xy', scale_units='xy', scale=1, color='limegreen', zorder=20, width=0.015)\n",
    "\n",
    "# plot the scaled eigenvectors\n",
    "ax1.quiver(np.real(lam[0])*vecs[0,0], np.real(lam[0])*vecs[1,0], angles='xy', scale_units='xy', \n",
    "           scale=1, color='g', zorder=30)\n",
    "ax1.quiver(np.real(lam[1])*vecs[0,1], np.real(lam[1])*vecs[1,1], angles='xy', scale_units='xy', \n",
    "           scale=1, color='g', zorder=30)\n",
    "\n",
    "ax1.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "The blue arrows are the original basis vectors, the red are what they transform to (the colums of $A$ here) - we've seen this sort of thing already. \n",
    "\n",
    "\n",
    "The blue dots show a subset of the points (at a finite resolution - `theta[::5]` in the code above \"prunes\" the number of points), showing how unit vectors or points on the initial unit circe map (in blue) map to new locations indicated by the red dots with black likes in between clarifying the correspondence.\n",
    "\n",
    "Note how  the new locations/vectors now map out a (non-unit) ellipse (some stretching and rotation has gone on).\n",
    "\n",
    "The green arrow indicate the *special* vectors/directions that do not change their orientation, only their magnitude. These are clearly the eigenvectors.\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's use matrix diagonalisation to break up this single transformation into three:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "    \n",
    "By noting that \n",
    "\n",
    "\\begin{align*}\n",
    "A = P \\Lambda P^{-1} \n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "\n",
    "$$P = \\left[ \\; \\boldsymbol{v}_1 \\;\\;  \\boldsymbol{v}_2  \\; \\; \\cdots \\; \\;  \\boldsymbol{v}_n \\; \\right]$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\Lambda = \\begin{pmatrix}\n",
    "\\lambda_1 & 0 &  \\cdots & 0 \\\\\n",
    "0 & \\lambda_2 &  \\ddots & 0\\\\\n",
    "\\vdots & &  \\ddots  & \\vdots \\\\\n",
    "0 & \\cdots & 0 & \\lambda_n\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "let's break down the single matrix multiplication $A\\boldsymbol{x}$ into three multiplications taken in turn:\n",
    "\n",
    "\\begin{align*}\n",
    "A\\boldsymbol{x} = P \\Lambda P^{-1}\\boldsymbol{x} = P \\; (\\Lambda\\;  (P^{-1}\\boldsymbol{x}))\n",
    "\\end{align*}\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2.2,0.3],[1.7,2.5]])\n",
    "\n",
    "# it's eigenvalues and eigenvectors\n",
    "lam1 = 3.079726\n",
    "v1 = np.array([0.322764, 0.946479])\n",
    "lam2 = 1.620274\n",
    "v2 = np.array([-0.459594,0.888129])\n",
    "\n",
    "# normalise the eigenvectors\n",
    "v1 = v1/np.sqrt(np.dot(v1,v1))\n",
    "v2 = v2/np.sqrt(np.dot(v2,v2))\n",
    "               \n",
    "# in order to form the matrix P, forst force v1 and v2 to be column vecs\n",
    "v1.shape = (2,1)\n",
    "v2.shape = (2,1)\n",
    "\n",
    "# and stack them into the matrix P using hstack\n",
    "P = np.hstack((v1,v2))\n",
    "# and form the diagonal matrix with e'vals on the main diagonal\n",
    "Lambda = np.diag(np.array([lam1,lam2]))\n",
    "\n",
    "# confirm our diagonalisation works\n",
    "print('A =  P@Lambda@sl.inv(P) :', np.allclose(A, P@Lambda@sl.inv(P)))\n",
    "print((P))\n",
    "print(Lambda)\n",
    "print(sl.inv(P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "First let's plot what the first multiplication\n",
    "\n",
    "$$P^{-1}\\boldsymbol{x}$$\n",
    "\n",
    "does to the unit circle:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# construct points on a circle parametrically\n",
    "theta = np.linspace(0,2*np.pi,100)\n",
    "np.append(theta, 0) # add a zero at the end so we go back to the start in the plot\n",
    "\n",
    "# A 2xn array of points\n",
    "points = np.vstack( (np.cos(theta), np.sin(theta)) )\n",
    "\n",
    "ax1.set_xlabel(\"$x, x'$\", fontsize=14)\n",
    "ax1.set_ylabel(\"$y, y'$\", fontsize=14)\n",
    "ax1.set_title('Linear transformation', fontsize=14)\n",
    "ax1.grid(True)\n",
    "\n",
    "x = np.array([1,0])\n",
    "y = np.array([0,1])\n",
    "\n",
    "# plot the circle\n",
    "ax1.plot(points[0,:], points[1,:], color='blue')\n",
    "\n",
    "# plot some vectors\n",
    "ax1.quiver(x[0], x[1], angles='xy', scale_units='xy', scale=1, color='b', zorder=10)\n",
    "ax1.quiver(y[0], y[1], angles='xy', scale_units='xy', scale=1, color='b', zorder=10)\n",
    "\n",
    "\n",
    "# Use our P^{-1} matrix\n",
    "\n",
    "xp = sl.inv(P)@x\n",
    "yp = sl.inv(P)@y\n",
    "pointsp = sl.inv(P)@points\n",
    "\n",
    "# plot the transformed circle\n",
    "ax1.plot(pointsp[0,:], pointsp[1,:], color='red')\n",
    "\n",
    "# plot the transformed vectors\n",
    "ax1.quiver(xp[0], xp[1], angles='xy', scale_units='xy', scale=1, color='r', zorder=10)\n",
    "ax1.quiver(yp[0], yp[1], angles='xy', scale_units='xy', scale=1, color='r', zorder=10)\n",
    "\n",
    "ax1.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "From above $P^{-1}$ is\n",
    "\n",
    "$$\\begin{pmatrix} \n",
    "1.230688 & 0.636863 \\\\\n",
    "-1.311544 & 0.447257\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "and so $(1,0)$ transform to $(1.230688,-1.311544)$ and similarly for the second unit vector transforming to the second column of $P^{-1}$.\n",
    "\n",
    "Note that in the case of a symmetric matrix $A$, $P$ is orthogonal and so this process we are mid way through here has a simpler interpretation - the $P$ and $P^{-1}$ matrices just perform rotations.\n",
    "\n",
    "<br>\n",
    "\n",
    "Now let's pre-multiply by $\\Lambda$:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before running this cell please make sure you run the previous\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# Use the points and the x,y from the previous cell:\n",
    "x = xp\n",
    "y = yp\n",
    "points = pointsp\n",
    "\n",
    "ax1.set_xlabel(\"$x, x'$\", fontsize=14)\n",
    "ax1.set_ylabel(\"$y, y'$\", fontsize=14)\n",
    "ax1.set_title('Linear transformation', fontsize=14)\n",
    "ax1.grid(True)\n",
    "\n",
    "# plot the circle\n",
    "ax1.plot(points[0,:], points[1,:], color='blue')\n",
    "\n",
    "# plot some vectors\n",
    "ax1.quiver(x[0], x[1], angles='xy', scale_units='xy', scale=1, color='b', zorder=10)\n",
    "ax1.quiver(y[0], y[1], angles='xy', scale_units='xy', scale=1, color='b', zorder=10)\n",
    "\n",
    "# Now use our Lambda matrix\n",
    "\n",
    "xp = Lambda@x\n",
    "yp = Lambda@y\n",
    "pointsp = Lambda@points\n",
    "\n",
    "# plot the transformed circle\n",
    "ax1.plot(pointsp[0,:], pointsp[1,:], color='red')\n",
    "\n",
    "# plot the transformed vectors\n",
    "ax1.quiver(xp[0], xp[1], angles='xy', scale_units='xy', scale=1, color='r', zorder=10)\n",
    "ax1.quiver(yp[0], yp[1], angles='xy', scale_units='xy', scale=1, color='r', zorder=10)\n",
    "\n",
    "ax1.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "The blue here was the outcome of the first operation, the red the new result.\n",
    "\n",
    "So pre-multiplication by the diagonal $\\Lambda$ just stretches in the $x$ and $y$ directions.\n",
    "\n",
    "<br>\n",
    "\n",
    "Finally we need to pre-multiply by $P$:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before running this cell please make sure you run the previous\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# Use the points and the x,y from the previous cell:\n",
    "x = xp\n",
    "y = yp\n",
    "points = pointsp\n",
    "\n",
    "ax1.set_xlabel(\"$x, x'$\", fontsize=14)\n",
    "ax1.set_ylabel(\"$y, y'$\", fontsize=14)\n",
    "ax1.set_title('Linear transformation', fontsize=14)\n",
    "ax1.grid(True)\n",
    "\n",
    "# plot the circle\n",
    "ax1.plot(points[0,:], points[1,:], color='blue')\n",
    "\n",
    "# plot some vectors\n",
    "ax1.quiver(x[0], x[1], angles='xy', scale_units='xy', scale=1, color='b', zorder=10)\n",
    "ax1.quiver(y[0], y[1], angles='xy', scale_units='xy', scale=1, color='b', zorder=10)\n",
    "\n",
    "# Now use our P matrix\n",
    "\n",
    "xp = P@x\n",
    "yp = P@y\n",
    "pointsp = P@points\n",
    "\n",
    "# plot the transformed circle\n",
    "ax1.plot(pointsp[0,:], pointsp[1,:], color='red')\n",
    "\n",
    "# plot the transformed vectors\n",
    "ax1.quiver(xp[0], xp[1], angles='xy', scale_units='xy', scale=1, color='r', zorder=10)\n",
    "ax1.quiver(yp[0], yp[1], angles='xy', scale_units='xy', scale=1, color='r', zorder=10)\n",
    "\n",
    "ax1.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "Again the blue was the output from the previous operations, and red the output from the final pre-multiplication.\n",
    "\n",
    "Note that (as long as you run the three cells in order!) the red result here agrees with the transformation under the matrix $A$ that we plotted all in one go at the start.\n",
    "\n",
    "<br>\n",
    "\n",
    "As already said, in the case of a ***symmetric matrix*** the $P$ and thus $P^{-1}$ being ***orthogonal matrices means that they just correspond to simple rotations***. In the case where we start from a symmetric matrix therefore the above three operations are easier to interpret - see homework for a worked example.\n",
    "\n",
    "Note that a homework exercise shows how we can recover the interpretation of matrix multiplication as a rotation and scaling and then another rotation if we instead decompose the matrix using a singlular value decomposition (SVD) rather than the eigendecomposition. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "Note that eigenvalues/vectors, and hence the eigen-decomposition of a matrix, are only defined for square matrices. \n",
    "\n",
    "The [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) generalises the eigen-decomposition concept to non-square matrices.\n",
    "\n",
    "Consider an arbitrary $m \\times n$ real matrix $A$ - very similar to the square case it can be decomposed into a product of three matrices:\n",
    "\n",
    "$$A = U\\Sigma V^{T}$$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "- $U$ is an $m\\times m$ orthogonal matrix whose columns are the eigenvectors of the matrix $AA^T$,\n",
    "\n",
    "\n",
    "- $V$ is an $n\\times n$ orthogonal matrix whose columns are the eigenvectors of the matrix $A^TA$,\n",
    "\n",
    "\n",
    "- $\\Sigma$ is an $m\\times n$ diagonal matrix whose diagonal entries, $\\sigma_1, \\sigma_2,\\ldots  $ are the ***singular values*** of $A$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "The singular values of $A$ are positive, and the convention is to number them/place down the diagonal of $\\Sigma$ in order of their magnitude: $\\sigma_1\\geq\\sigma_2\\geq\\ldots\\geq 0$.\n",
    "\n",
    "<br>\n",
    "\n",
    "The singular values are the square roots of the eigenvalues of the square matrix $A^TA$.  They are also the square roots of the eigenvalues of the square matrix $AA^T$.  \n",
    "\n",
    "But hang on .....  if $A$ is non-square then $A^TA$ and $AA^T$ are of different size and thus have different numbers of eigenvalues - how to reconcile this apparent contradiction? -> The extra eigenvalues are always zero (check this in the homework), i.e. the maximum number of non-zero singular values is the smaller of $m$ and $n$.\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that  we said we wanted an approach that worked when $A^TA$ or $AA^T$ are not invertible (equivalently have a zero eigenvalue - this is fine, the SVD handles arbitrarily many zero singular values as an extension of the previous comment on the \"extra eigenvalues\".)\n",
    "\n",
    "<br>\n",
    "\n",
    "*(Figure: https://en.wikipedia.org/wiki/Singular_value_decomposition#/media/File:Singular_value_decomposition_visualisation.svg)*\n",
    "\n",
    "<img src=\"./figures/Singular_value_decomposition_visualisation.svg\" width=400x>\n",
    "\n",
    "\n",
    "replace the star with transpose for cases we will consider here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "## Some SVD properties [*]\n",
    "\n",
    "\n",
    "- The rank of $A$ is $p$, where $p$ is the number of non-zero singular values.\n",
    "\n",
    "\n",
    " \n",
    "- The first $p$ columns of $U$ form a basis for the range of $A$ (i.e. the span of the columns of $A$); the final $n-p$ columns of $V$ form a basis for the null space of $A$\n",
    " \n",
    " \n",
    "\n",
    "- $\\|A\\|_2 = \\sigma_1$ and $\\|A\\|_F = \\sqrt{\\sigma_1^2+\\sigma_2^2+\\ldots+\\sigma_p^2}$.\n",
    " \n",
    "\n",
    "\n",
    "- If $A = A^{T}$, then the singular values of $A$ are the absolute values of the eigenvalues of $A$ (the absolute values coming from squaring and taking the square root)\n",
    "\n",
    "\n",
    " \n",
    "- The absolute value of the determinant of a square matrix $A$ is equal to the product of its singular values.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric interpretation\n",
    "\n",
    "Above we showed that, via the eigendecomposition, we can interpret the action of matrix multiplication geometrically - by how it deforms a sphere (it turns it into a hyper-ellipse). \n",
    "\n",
    "In the ***square*** ***symmetric*** case the orthonormal eigenvectors describe rotations and the eigenvalues scalings.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "You can think of SVD as the generalisation of this - allowing us to geometrically understand the action of matrix multiplication for a non-square matrix (and a square but non-symmetric matrix) as a series of rotations (as $U$ and $V$ are orthogonal) and scalings:\n",
    "\n",
    "\n",
    "\n",
    "*(Figure: https://en.wikipedia.org/wiki/Singular_value_decomposition#/media/File:Singular-Value-Decomposition.svg)*\n",
    "\n",
    "<img src=\"./figures/Singular-Value-Decomposition.svg\" width=400x>\n",
    "\n",
    "\"... the singular values of any $m \\times n$ matrix can be viewed as the magnitude of the semiaxis of an $n$-dimensional ellipsoid in $m$-dimensional space, for example as an ellipse in a (tilted) 2D plane in a 3D space. Singular values encode magnitude of the semiaxis, while singular vectors encode direction. ...\"\n",
    "\n",
    "\n",
    "The size and shape of $\\Sigma$, and the zeros that appear in the singular values, are responsible for the embeddings that occur as we map between spaces with different dimensions.\n",
    "\n",
    "An example of a $3\\times 2$, i.e. mapping $\\mathbb{R}^2$ into $\\mathbb{R}^3$ is given in the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The compact form of SVD\n",
    "\n",
    "If both $A^TA$ and $AA^T$ are singular, then some of the singular values will be zero.  \n",
    "\n",
    "Let us suppose that there are $p$ non-zero singular values.  \n",
    "\n",
    "$A$ can then also be decomposed in a more compact (or reduced) form using only the eigenvectors that correspond to non-zero values:\n",
    "\n",
    "$$A = U_p\\Sigma_p V_p^{T}$$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "- $U_p$ is an $m\\times p$ orthogonal matrix whose columns are the eigenvectors of the matrix $AA^T$,\n",
    "\n",
    "\n",
    "- $V_p$ is an $n\\times p$ orthogonal matrix whose columns are the eigenvectors of the matrix $A^TA$,\n",
    "\n",
    "\n",
    "- $\\Sigma_p$ is an $p\\times p$ diagonal matrix whose diagonal entries, $\\sigma_1, \\sigma_2, \\ldots , \\sigma_p>0$  are the non-zero *singular values* of $A$\n",
    "\n",
    "<br>\n",
    "\n",
    "Here we just don't bother storing those entries which due to the zero singular values never contribute to the multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "## Aside: some of the various uses of the SVD\n",
    "\n",
    "1. This geometrical interpretation demonstrates how we can use the SVD to appreciate the most important direction of uncertainty/noise in observations - important in that they can have large or small impact on the model parameters we arrive at when doing inversion.  In  a very high dimensional system, if we wanted to perturb our model parameters (e.g. the initial state of the atmosphere in a weather forecast), then we can't afford to run lots of forecasts (termed an ensemble) where we fully explore the uncertainty, instead we might perturb the model parameters in the directions associated with the largest singular values.  Take a look at the homework exercise that demonstrates this by plotting scaled singular vectors.\n",
    "For more on this in the context of weather forecasting see: [Singular vectors in atmospheric sciences: A review](<https://www.sciencedirect.com/science/article/pii/S0012825212000657)\n",
    "\n",
    "\n",
    "\n",
    "2. The SVD is very important in related field, e.g. see a description here of its importance in Machine Learning <https://medium.com/@jonathan_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491>>\n",
    "\n",
    "\n",
    "\n",
    "3. Since we can interpret an image in terms of a matrix, and a fun application of SVD is in image compression.  Notice first that expanding out the SVD:\n",
    "$$A = U\\Sigma V^{T} = \\sigma_1 u_{:1} v_{:1}^T +  \\sigma_2 u_{:s} v_{:s}^T + \\ldots $$\n",
    "since the $\\sigma$'s are ordered in terms of their magnitudes, if we truncate this expansion early we get a so-called [low rank approximation](https://en.wikipedia.org/wiki/Low-rank_approximation) to the original matrix, or if we interpret the matrix as an image, we get different levels of compressed image! See a homework exercise and <https://medium.com/@rameshputalapattu/jupyter-python-image-compression-and-svd-an-interactive-exploration-703c953e44f6>\n",
    "\n",
    "\n",
    "4. It is closely related to Principal Component Analysis some of you may have heard about <https://math.stackexchange.com/questions/3869/what-is-the-intuitive-relationship-between-svd-and-pca>  and [A Tutorial on Principal Component Analysis](https://arxiv.org/pdf/1404.1100.pdf)\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "5. And finally, it is the basis for a type of inverse of use to us in this module:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The generalised/pseudo/Moore-Penrose inverse\n",
    "\n",
    "## Definition\n",
    "\n",
    "So we know that we have the compact form of the SVD for any matrix $A$:\n",
    "\n",
    "\n",
    "$$A = U_p\\Sigma_p V_p^{T}$$\n",
    "\n",
    "\n",
    "The ***generalised (or pseudo, or Moore-Penrose) inverse*** of $A$, written $A^+$ is defined as\n",
    "\n",
    "\n",
    "$$A^+ = V_p\\Sigma_p^{-1} U_p^{T}$$\n",
    "\n",
    "\n",
    "Note that $\\Sigma_p$  is a diagonal matrix with non-zero entries on the diagonal, so that its inverse is easily generated simply by reciprocating each of its diagonal elements.  For $U$ and $V$ we have just transposed and swapped their order. Therefore forming this inverse once we have the (compact form of the) SVD is a triviality.\n",
    "\n",
    "\n",
    "## Properties\n",
    "\n",
    "The generalised inverse has the following properties all of which make it behave in ways that are similar to a true inverse: \n",
    "\n",
    "\n",
    "$$\\begin{align*}\n",
    "AA^+A &= A\\\\\n",
    "A^+AA^+ &= A^+\\\\\n",
    "(A^+A)^T &= A^+A\\\\\n",
    "(AA^+)^T &= AA^+\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that the generalised inverse defined above exists for any matrix other than the zero matrix, but when the matrix is of full rank $\\text{rank}(A)=\\min(m,n)$) then it can be expressed via simple algebraic formulae that we've seen before!\n",
    "\n",
    "\n",
    "\n",
    "- If $A$ is square and non-singular, then $A^+=A^{-1}$, so that it is the regular inverse of $A$. \n",
    "\n",
    "\n",
    "\n",
    "- If $A^TA$ is non-singular, then the generalised inverse defined above is equivalent to $A^+=(A^TA)^{-1}A^T$, so that it is the least-squares inverse of $A$ (this is the \"tall\" over-determined case where the columns are linearly independent - the left inverse).\n",
    "\n",
    "\n",
    "\n",
    "- If $AA^T$ is non-singular, then $A^+=A^T(AA^T)^{-1}$, so that it is the minimum-norm inverse of $A$ (this is the \"fat\" under-determine case, where the rows are linearly independent - the right inverse).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "$A^+$ always exists for any non-null (non-zero) matrix $A$ so that the generalised inverse solution $\\boldsymbol{x}^+$ to the equation $A\\boldsymbol{x} = \\boldsymbol{b}$ also always exists and is given by  \n",
    "\n",
    "\n",
    "$$\\boldsymbol{x}^+ = A^+\\boldsymbol{b}$$\n",
    "\n",
    "\n",
    "It has the properties that\n",
    "\n",
    "\n",
    "- If there is any model parameter vector that can provide an exact fit to the data $\\boldsymbol{b}$, then $\\boldsymbol{x}^+$  will be such a vector. \n",
    "\n",
    "\n",
    "\n",
    "- If there is no model parameter vector that can provide an exact fit to the data $\\boldsymbol{b}$, then $\\boldsymbol{x}^+$ will provide a best least-squares fit to the data.  \n",
    "\n",
    "\n",
    "   \n",
    "- If there is more than one exact or best-fitting parameter vector to the data $\\boldsymbol{b}$, then $\\boldsymbol{x}^+$ will be the exact or best-fitting model that also has the smallest norm. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Thus, we can apply the generalised inverse to provide a solution to ***any*** linear inverse, and we will get a result that has various sensible and desirable properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [1,0,0],\n",
    "    [1,0,0],\n",
    "    [0,2,2],\n",
    "    [0,3,3]])\n",
    "b = np.array([1,2,2,3])\n",
    "\n",
    "# note that numpy has a pseudoinverse function \n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html\n",
    "\n",
    "pinvA = np.linalg.pinv(A)\n",
    "\n",
    "x = pinvA @ b\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(A@x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "## Problems [*]\n",
    "\n",
    "The generalised inverse however has some problems: \n",
    "\n",
    "<br>\n",
    "\n",
    "The generalised inverse is expensive to compute (which is why the algebraic expressions in the case of a full rank matrix are useful) - the SVD calculation is expensive for large matrices and in practical terms it is impossible to use for realistic (i.e. large) inverse problems.   \n",
    "\n",
    "<br>\n",
    "\n",
    "Although  it  is  able  to  deal  with  zero-valued  singular  values  and  does  not  suffer  from problems when matrices are singular, it has no protection against very small singular values or very small eigenvalues that may be practically indistinguishable from zero when using finite-precision arithmetic.  \n",
    "\n",
    "If the ratio of the largest to the smallest singular value is very large, then just as the case we've already seen for square problems, the system will be ***ill-conditioned***.  These small singular values will dominate the generalised inverse because they are reciprocated and become very large when calculating the inverse of $\\Sigma$.  In real datasets, the actualy values of the small singular values and small eigenvalues calculated will typically be dominated by noise.  The generalised inverse then will act to amplify the separate noise that will be present in the data, and in very poorly conditioned problems, the resulting model may be entirely dominated by noise - the model will fit the data and have a minimum norm, but it will nonetheless be nonsense as errors will have been massively amplified!\n",
    "\n",
    "<br>\n",
    "\n",
    "Also, most real inverse problems  are  non-linear,  and  the  generalised  inverse  only  solves  linear problems.  While the generalised inverse could be used within an iterative approach where a linearised version of the non-linear problem is solved at each iteration, it is likely to be overkill for that approach.  When we iterate to solve a non-linear problem, we generally do not need an exact and hence expensive solution at each step,  we just need a reasonable solution that improves the model, and then iteration will take care  of the rest.  In linearised inversion, the linear problem that we solve at each step is not the correct problem, it is just an approximation to it - consequently it is not sensible to spend huge effort to get an exact solution to what is only an approximation to the true problem.  \n",
    "\n",
    "You'll see more on this in later lectures.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "    \n",
    "# Regularisation [*]\n",
    "\n",
    "If we have a mixed-determined problem for which the generalised inverse is not affordable, or for which small singular values and small eigenvalues lead to a noisy model, then we will need a different approach.  \n",
    "\n",
    "In the case of a square problem, the matrix may not be singular, but it may still have a very small determinant and at least one very small eigenvalue.  In such a case, although the inverse may exist, the problem is unstable - large spurious values will likely appear in the solution.  The solution  will  depend  upon  the  values  of  the  smallest  eigenvalues,  which  themselves  will likely  be  almost  entirely  related  to  the  noise  in  the  system.    Effectively,  in  any  inversion problem, we face the issue of potentially dividing by small numbers that are noise dominated. This is related to the brief mention of ill-conditioning from earlier as well as in ACSE2/3.\n",
    "\n",
    "There are several related methods for dealing with this situation.   Here we will explore damped least-squares and from  there introduce general schemes for stabilisation and regularisation of the general inverse problem. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "\n",
    "## Damped least squares [*]\n",
    "\n",
    "In damped least-squares, instead of minimising the objective function of the form\n",
    "\n",
    "$$\n",
    "f=\\|\\boldsymbol{p} - \\boldsymbol{b}\\|_2^2\n",
    "= \\|A\\boldsymbol{x} - \\boldsymbol{b}\\|_2^2\n",
    "$$\n",
    "\n",
    "[earlier we minimsed this function (with a factor of 1/2 introduced purely for convenience)]\n",
    "\n",
    "instead we minimise the objective function\n",
    "\n",
    "$$\n",
    "f = \\|A\\boldsymbol{x} - \\boldsymbol{b}\\|_2^2 + \\mu \\|\\boldsymbol{x}\\|_2^2\n",
    "$$\n",
    "\n",
    "where $\\mu$ is  a  positive  scalar  known  in  various  contexts  as  the  damping  factor,  trade-off, regularisation or pre-whitening parameter. \n",
    "\n",
    "<br>\n",
    "\n",
    "Now when we invert, if you follow through the steps taken to derive least squares, the equation that we must solve takes the form \n",
    "\n",
    "$$ \\boldsymbol{x} = (A^TA + \\mu{I})^{-1} A^T \\boldsymbol{b}$$\n",
    "\n",
    "For a positive $\\mu$, note that the matrix $(A^TA + \\mu{I})$ is always non-singular and thus we are able to use this expression - it can be used to solve equi-, under-, over- and mixed-determined systems even if $A^TA$ and $AA^T$ are singular.\n",
    "\n",
    "<br>\n",
    "\n",
    "By damping the least squares system, we have protected the solution against zero or small eigenvalues by adding a constant value, controlled by $\\mu$, to the diagonal of $A^TA$. If $\\mu$ is very small,  then  we  will  have something very close to  regular  least  squares (cf. the under-determined case above);  if  $\\mu$  is  very  large, then  we  will  simply minimise the norm of the model parameters (the second term dominating the first).  Choosing the value of $\\mu$ therefore involves a  trade-off between fitting the data and stabilising the result.  \n",
    "\n",
    "We saw earlier that the minimal-norm inverse involved first finding an exact or best least-squares fit to the data, then, subject to that constraint, finding the solution that minimised the $L^2$-norm of the model/solution vector (cf. homework exercise - \"minimal-norm solution to under-determined problem\").\n",
    " \n",
    "Damped  least-squares  does  something  similar,  except  here  the parameter $\\mu$ is adjusted to control the degree to which the solution fits the data (which is favoured by decreasing the value of $\\mu$), and the degree to which it  minimises  the  norm  of  the  model (which is favoured by increasing the value of $\\mu$). Recall that in practice a problem may well be formulated such that it is the perturbation to an *a priori* model rather than the model itself that is minimised. \n",
    "\n",
    "The key idea here is to set the value of $\\mu$ so that it removes the influence of small eigenvalues in $A^TA$ from the problem, but leaves the larger eigenvalues alone.  The larger eigenvalues then control the fit to the data (the smaller eigenvalues would contribute almost nothing to this  fit  anyway  because  they  are  small),  and  the  resulting  under-determined model is then fully determined by minimising its norm.   \n",
    "\n",
    "Including only  the  largest  eigenvalues  in  the  solution  will effectively  reduce  the  resolution  of  the recovered  model  parameters,  whereas  including  the  smaller  eigenvalues  will  increase  the noise  within  the  recovered  model  parameters.    We  are  therefore  trading  resolution  against noise in the model, and the value of $\\mu$ controls that trade-off.  Yet another way to regard $\\mu$ is that we should choose it so that the fit to the data is only as good as is required by the level of noise in the data, but no better.  If we insist on lowering $\\mu$ beyond this, then we will be [***over-fitting***](https://en.wikipedia.org/wiki/Overfitting) the data - the increased apparent match to the observed data will then merely serve to map noise from the data into the model without additional benefit to the model. \n",
    "\n",
    "In practice, it can be difficult to choose $\\mu$ to achieve these outcomes.  One way to do this, at least in principle, is to invert the data using a range of values for $\\mu$, and then plot the model norm against the data-residual norm as in \n",
    "\n",
    "<img src=\"./figures/trade_off_curve.png\" width=400x>\n",
    "\n",
    "\n",
    "If we are lucky, there may be a corner or elbow in this plot that indicates a clear break and an optimal value for $\\mu$; in many problems though no obvious  corner  appears.   If  we  know  the  data  errors,  then  we  can  compute  the  minimum useful value of the data-residual norm, and select a value of $\\mu$ that is consistent with this.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"optional\">\n",
    "    \n",
    "## More general regularisation [*]\n",
    "\n",
    "Use of damped least-squares assumes implicitly that we know something about the model parameters that is independent  of  the data - that is we know it *a  priori*, before we undertake the experiment or observations that generate the data.  \n",
    "In this case, our *a priori* knowledge is simply that the model should have a smal  norm, ideally zero.    \n",
    "This assumption is most appropriate when, in the absence of data, we would assume that the model was zero - typically this will be the case when the model for which we are inverting is the perturbation to some best-guess starting model.\n",
    "\n",
    "However, assuming a minimum-norm is not the only a priori assumption we can make. \n",
    "We might instead assume for example that the norm of the first or second  derivatives of the model parameters should be small.   \n",
    "The first of these is the assumption that the model is  \"flat\" - all the model parameters are the same - and the second is the assumption that the model is \"smooth\" - the variation between adjacent parameters is everywhere the same.   \n",
    "\n",
    "These assumptions, and the assumption of minimal model norm as made in the damped least-squares approach, are all forms of [***model regularisation***](https://en.wikipedia.org/wiki/Regularization_(mathematics)).  \n",
    "In this case, they are a type of regularisation called [***Tikhonov regularisation***](https://en.wikipedia.org/wiki/Tikhonov_regularization). \n",
    "\n",
    "Minimising the model norm is sometimes called zero-order Tikhonov regularisation, and minimising the norms of the first and second differentials of the model are called first and second-order Tikhonov regularisation.  \n",
    "\n",
    "There are many other forms of regularisation possible, but these three illustrate the general idea. \n",
    "\n",
    "Regularisation in general refers to the process of introducing additional a priori information about the properties of the model in order to solve ill-posed problems without over-fitting the data and without introducing noise, instability and spurious structure into the resultant model. \n",
    "\n",
    "<br>\n",
    "\n",
    "Many regularisation schemes can be implemented by minimising an objective function of the form \n",
    "\n",
    "$$\n",
    "f = \\|A\\boldsymbol{x} - \\boldsymbol{b}\\|_2^2 + \\mu \\|L\\boldsymbol{x}\\|_2^2,\n",
    "$$\n",
    "\n",
    "where $L$ is a matrix that acts on $\\boldsymbol{x}$ to generate the property that we seek to minimise. \n",
    "For damped least-squares, $L$ is simply the identity matrix $I$.  \n",
    "For first-order Tikhonov regularisation $L$ needs to operate of $\\boldsymbol{x}$ to generate a first order approximate derivative, e.g. using an upwind finite difference. \n",
    "In second-order Tikhonov regularisation, $L$ needs to act to generate an approximation to the second derivative of the model, e.g. a second-order central difference.  \n",
    "\n",
    "Of course in 1D it's easy to write down these operators as matrices. \n",
    "In higher dimensions things are more complicated, and depend on the ordering assumed for $\\boldsymbol{x}$.\n",
    "\n",
    "Note that through the additional of extra terms it's possible to incorporate multiple regularisation terms.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"optional\">\n",
    "    \n",
    "# Nonlinear problems [*]\n",
    "\n",
    "So far we have considered solution methods appropriate for small linear problems.\n",
    "\n",
    "Of course for real world applications we need to be prepared to deal with very large and very nonlinear problems. We potentially also need to incorporate various additional types of constraints on our solutions.\n",
    "\n",
    "Covered in the coming lectures.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this lecture we have introduced linear inversion problems.\n",
    "\n",
    "We have introduced the least squares and minimal-norm solution, and for problems with full rank matrices have derived formulae for these.\n",
    "\n",
    "Via the generalised inverse (and regularisation), we have introduced approaches that are appropriate for use with **all** types of linear inversion problems.\n",
    "\n",
    "In order to introduce the generalised inverse we needed to introduce the SVD, which was motivated and computed via eigenvalues/vectors/decompositions which were also reviewed here.\n",
    "\n",
    "We have noted challenges with the approaches developed so far in this module, namely how to deal with large and/or nonlinear problems.\n",
    "\n",
    "The remainder of this module will go through these topics in more detail.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Key take home points from this lecture:\n",
    "\n",
    "\n",
    "1. Optimisation and inversion - the \"big picture\" overview\n",
    "\n",
    "\n",
    "2. The least squares solution (for over-determined problems)\n",
    "\n",
    "\n",
    "3. The minimum norm solution (for under-determined problems)\n",
    "\n",
    "\n",
    "4. Matrix rank, null space etc\n",
    "\n",
    "\n",
    "5. The SVD (and the generalised inverse)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
